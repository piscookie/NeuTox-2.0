{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80eac477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [00:18<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "file = 'NT'\n",
    "\n",
    "file_prefixes = [\"smiles\",\"DMPNN\", \"ecfp\", \"mfbert\", \"padel\"]\n",
    "file_extension = \".csv\"\n",
    "\n",
    "file_prefix = f\"{file}_Feature_fusion/data_train\"\n",
    "file_extension = \".csv\"\n",
    "\n",
    "val_file_prefix = f\"{file}_Feature_fusion/data_valid\"\n",
    "val_file_extension = \".csv\"\n",
    "\n",
    "start_index = 0\n",
    "end_index = 4\n",
    "\n",
    "auc_scores = []\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(start_index, end_index + 1), desc=\"Processing files\"):  # 使用 tqdm 添加进度条  \n",
    "    combined_data_train = None\n",
    "    combined_data_valid = None\n",
    "    combined_data_test = None\n",
    "    for file_feature in file_prefixes:\n",
    "        #从randomi/中获取train，valid，test    \n",
    "        \n",
    "        file_path_train = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"train\" + file_extension\n",
    "        data_train = pd.read_csv(file_path_train) \n",
    "                \n",
    "        file_path_valid = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"valid\" + file_extension\n",
    "        data_valid = pd.read_csv(file_path_valid) \n",
    "\n",
    "        file_path_test = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"test\" + file_extension\n",
    "        data_test = pd.read_csv(file_path_test)   \n",
    "        \n",
    "        data_train = pd.read_csv(file_path_train)     \n",
    "        data_valid = pd.read_csv(file_path_valid)   \n",
    "        data_test  = pd.read_csv(file_path_test) \n",
    "        \n",
    "        if combined_data_train is None:\n",
    "            combined_data_train = data_train\n",
    "        if combined_data_valid is None:\n",
    "            combined_data_valid = data_valid\n",
    "        if combined_data_test is None:   \n",
    "            combined_data_test  = data_test\n",
    "        else:\n",
    "            combined_data_train = pd.concat([combined_data_train, data_train], axis=1)     \n",
    "            combined_data_valid = pd.concat([combined_data_valid, data_valid], axis=1) \n",
    "            combined_data_test = pd.concat([combined_data_test, data_test], axis=1) \n",
    "    \n",
    "    folder_path = f'{file}_Feature_fusion'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    combined_data_train.to_csv(f'{file}_Feature_fusion/data_train{i}.csv', index=False)\n",
    "    combined_data_valid.to_csv(f'{file}_Feature_fusion/data_valid{i}.csv', index=False)\n",
    "    combined_data_test.to_csv(f'{file}_Feature_fusion/data_test{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a732548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据中没有空值\n",
      "该文件的行数为：2087，列数为：3538\n",
      "数据中没有空值\n",
      "该文件的行数为：2087，列数为：3538\n",
      "数据中没有空值\n",
      "该文件的行数为：2087，列数为：3538\n",
      "数据中没有空值\n",
      "该文件的行数为：2087，列数为：3538\n",
      "数据中没有空值\n",
      "该文件的行数为：2087，列数为：3538\n"
     ]
    }
   ],
   "source": [
    "file_prefix = f\"{file}_Feature_fusion/data_train\"\n",
    "file_extension = \".csv\"\n",
    "\n",
    "val_file_prefix = f\"{file}_Feature_fusion/data_valid\"\n",
    "val_file_extension = \".csv\"\n",
    "\n",
    "for i in range(start_index, end_index+1):\n",
    "    \n",
    "    # 读取数据\n",
    "    file_path = file_prefix + str(i) + file_extension\n",
    "    data = pd.read_csv(file_path)\n",
    "    null_indices = None  # 初始化 null_indices 为 None\n",
    "    if data.isnull().values.any():  # 判断是否有空值\n",
    "        print(\"数据中存在空值，空值坐标为：\")\n",
    "        null_indices = np.where(data.isnull())  # 获取空值的坐标\n",
    "        if null_indices is not None:  # 再次判断 null_indices 是否有值\n",
    "            for row_index, col_index in zip(null_indices[0], null_indices[1]):\n",
    "                print(f\"行 {row_index}，列 {col_index}\")\n",
    "    else:\n",
    "        print(\"数据中没有空值\")\n",
    "    print(f\"该文件的行数为：{data.shape[0]}，列数为：{data.shape[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "802b00cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES    OC(CNCC1=CC=CC=C1)COCC2=CC=CC=C2\n",
      "active                                   1\n",
      "0                                      0.0\n",
      "1                                      0.0\n",
      "2                                 0.088121\n",
      "                        ...               \n",
      "1439                              0.094399\n",
      "1440                              0.028085\n",
      "1441                              0.162963\n",
      "1442                              0.610029\n",
      "1443                              0.235897\n",
      "Name: 0, Length: 3538, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2595, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2594864070415497, Val Loss: 0.2422243058681488\n",
      "model_0训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2422, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.2422243058681488, Val Loss: 0.22606632113456726\n",
      "model_0训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2261, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.22606632113456726, Val Loss: 0.21942368149757385\n",
      "model_0训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2194, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.21942368149757385, Val Loss: 0.22315724194049835\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2232, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.22315724194049835, Val Loss: 0.22331464290618896\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2233, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.22331464290618896, Val Loss: 0.21861079335212708\n",
      "model_0训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2186, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.21861079335212708, Val Loss: 0.21323677897453308\n",
      "model_0训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.2132, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.21323677897453308, Val Loss: 0.2100294679403305\n",
      "model_0训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.2100, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.2100294679403305, Val Loss: 0.2094441056251526\n",
      "model_0训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.2094, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.2094441056251526, Val Loss: 0.20772771537303925\n",
      "model_0训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.2077, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.20772771537303925, Val Loss: 0.2036498337984085\n",
      "model_0训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.2036, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.2036498337984085, Val Loss: 0.19929440319538116\n",
      "model_0训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.1993, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.19929440319538116, Val Loss: 0.19509997963905334\n",
      "model_0训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1951, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.19509997963905334, Val Loss: 0.19080674648284912\n",
      "model_0训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1908, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.19080674648284912, Val Loss: 0.18545927107334137\n",
      "model_0训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1855, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.18545927107334137, Val Loss: 0.17894166707992554\n",
      "model_0训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1789, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.17894166707992554, Val Loss: 0.17210224270820618\n",
      "model_0训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.1721, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.17210224270820618, Val Loss: 0.16519950330257416\n",
      "model_0训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.1652, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.16519950330257416, Val Loss: 0.15717600286006927\n",
      "model_0训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.1572, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.15717600286006927, Val Loss: 0.15009720623493195\n",
      "model_0训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.1501, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.15009720623493195, Val Loss: 0.1426478624343872\n",
      "model_0训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.1426, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.1426478624343872, Val Loss: 0.1359477937221527\n",
      "model_0训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.1359, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.1359477937221527, Val Loss: 0.12835106253623962\n",
      "model_0训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.1284, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.12835106253623962, Val Loss: 0.12203385680913925\n",
      "model_0训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.1220, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.12203385680913925, Val Loss: 0.11539032310247421\n",
      "model_0训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.1154, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.11539032310247421, Val Loss: 0.10895812511444092\n",
      "model_0训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.1090, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.10895812511444092, Val Loss: 0.10187160223722458\n",
      "model_0训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.1019, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.10187160223722458, Val Loss: 0.09521611034870148\n",
      "model_0训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0952, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.09521611034870148, Val Loss: 0.0894288569688797\n",
      "model_0训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0894, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.0894288569688797, Val Loss: 0.08411470055580139\n",
      "model_0训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0841, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.08411470055580139, Val Loss: 0.07980223000049591\n",
      "model_0训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0798, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.07980223000049591, Val Loss: 0.07655735313892365\n",
      "model_0训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0766, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.07655735313892365, Val Loss: 0.07414747029542923\n",
      "model_0训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0741, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.07414747029542923, Val Loss: 0.07235704362392426\n",
      "model_0训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0724, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.07235704362392426, Val Loss: 0.07087915390729904\n",
      "model_0训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0709, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.07087915390729904, Val Loss: 0.06953473389148712\n",
      "model_0训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0695, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.06953473389148712, Val Loss: 0.06827449053525925\n",
      "model_0训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0683, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.06827449053525925, Val Loss: 0.06706415116786957\n",
      "model_0训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0671, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.06706415116786957, Val Loss: 0.06592115014791489\n",
      "model_0训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0659, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.06592115014791489, Val Loss: 0.06485263258218765\n",
      "model_0训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0649, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.06485263258218765, Val Loss: 0.06387118250131607\n",
      "model_0训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0639, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.06387118250131607, Val Loss: 0.0630212053656578\n",
      "model_0训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0630, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.0630212053656578, Val Loss: 0.06241744011640549\n",
      "model_0训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0624, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.06241744011640549, Val Loss: 0.06263306736946106\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0626, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.06263306736946106, Val Loss: 0.06502517312765121\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0650, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.06502517312765121, Val Loss: 0.0653320774435997\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0653, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.0653320774435997, Val Loss: 0.06072906404733658\n",
      "model_0训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0607, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.06072906404733658, Val Loss: 0.05965893343091011\n",
      "model_0训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0597, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Train Loss: 0.05965893343091011, Val Loss: 0.06173619627952576\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0617, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.06173619627952576, Val Loss: 0.05785660445690155\n",
      "model_0训练结果更新为第49个模型\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0579, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.05785660445690155, Val Loss: 0.058149319142103195\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0581, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.058149319142103195, Val Loss: 0.05829082429409027\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0583, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.05829082429409027, Val Loss: 0.05541408434510231\n",
      "model_0训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0554, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.05541408434510231, Val Loss: 0.05663150176405907\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0566, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.05663150176405907, Val Loss: 0.055218812078237534\n",
      "model_0训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0552, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.055218812078237534, Val Loss: 0.05399157106876373\n",
      "model_0训练结果更新为第55个模型\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0540, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.05399157106876373, Val Loss: 0.05479404330253601\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0548, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.05479404330253601, Val Loss: 0.05305316299200058\n",
      "model_0训练结果更新为第57个模型\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0531, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.05305316299200058, Val Loss: 0.052349694073200226\n",
      "model_0训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0523, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.052349694073200226, Val Loss: 0.0527939535677433\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0528, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.0527939535677433, Val Loss: 0.05139491334557533\n",
      "model_0训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.05139491334557533, Val Loss: 0.050198741257190704\n",
      "model_0训练结果更新为第61个模型\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0502, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.050198741257190704, Val Loss: 0.05034452676773071\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0503, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.05034452676773071, Val Loss: 0.050163716077804565\n",
      "model_0训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0502, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.050163716077804565, Val Loss: 0.04915865510702133\n",
      "model_0训练结果更新为第64个模型\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0492, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.04915865510702133, Val Loss: 0.047838591039180756\n",
      "model_0训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0478, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.047838591039180756, Val Loss: 0.047019682824611664\n",
      "model_0训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0470, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.047019682824611664, Val Loss: 0.046742767095565796\n",
      "model_0训练结果更新为第67个模型\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0467, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.046742767095565796, Val Loss: 0.047142352908849716\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0471, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.047142352908849716, Val Loss: 0.04964519664645195\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0496, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.04964519664645195, Val Loss: 0.05626098811626434\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0563, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.05626098811626434, Val Loss: 0.06297527253627777\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0630, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.06297527253627777, Val Loss: 0.04905810207128525\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0491, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.04905810207128525, Val Loss: 0.04600684344768524\n",
      "model_0训练结果更新为第73个模型\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0460, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.04600684344768524, Val Loss: 0.053922973573207855\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0539, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.053922973573207855, Val Loss: 0.04249813035130501\n",
      "model_0训练结果更新为第75个模型\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0425, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.04249813035130501, Val Loss: 0.049326010048389435\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0493, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.049326010048389435, Val Loss: 0.04550855979323387\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0455, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.04550855979323387, Val Loss: 0.04358426481485367\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.04358426481485367, Val Loss: 0.0462116003036499\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0462, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.0462116003036499, Val Loss: 0.0402449406683445\n",
      "model_0训练结果更新为第80个模型\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0402, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.0402449406683445, Val Loss: 0.04479803889989853\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0448, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.04479803889989853, Val Loss: 0.03923015296459198\n",
      "model_0训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0392, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.03923015296459198, Val Loss: 0.042805880308151245\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0428, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.042805880308151245, Val Loss: 0.03930786997079849\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0393, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.03930786997079849, Val Loss: 0.04012592136859894\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0401, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.04012592136859894, Val Loss: 0.03942980244755745\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0394, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.03942980244755745, Val Loss: 0.037476688623428345\n",
      "model_0训练结果更新为第87个模型\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0375, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.037476688623428345, Val Loss: 0.03905276954174042\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0391, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.03905276954174042, Val Loss: 0.03583415597677231\n",
      "model_0训练结果更新为第89个模型\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0358, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.03583415597677231, Val Loss: 0.03767260164022446\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0377, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.03767260164022446, Val Loss: 0.035678599029779434\n",
      "model_0训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0357, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.035678599029779434, Val Loss: 0.03505295515060425\n",
      "model_0训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0351, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.03505295515060425, Val Loss: 0.03582305461168289\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0358, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.03582305461168289, Val Loss: 0.03336859121918678\n",
      "model_0训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0334, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.03336859121918678, Val Loss: 0.033921077847480774\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0339, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.033921077847480774, Val Loss: 0.033749036490917206\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0337, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.033749036490917206, Val Loss: 0.03174538165330887\n",
      "model_0训练结果更新为第97个模型\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0317, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.03174538165330887, Val Loss: 0.032072391360998154\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0321, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.032072391360998154, Val Loss: 0.03218553587794304\n",
      "         \n",
      "第 0 组模型\n",
      "AUC: 0.9731475283538357\n",
      "ACC: 0.967896502156205\n",
      "F1: 0.9764499121265378\n",
      "Recall: 0.9809322033898306\n",
      "MCC: 0.9261347414254145\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES    NCCS(O)(=O)=O\n",
      "active                1\n",
      "0                   0.0\n",
      "1              0.008748\n",
      "2              0.074378\n",
      "              ...      \n",
      "1439           0.074249\n",
      "1440           0.001213\n",
      "1441           0.028169\n",
      "1442           0.524964\n",
      "1443           0.071795\n",
      "Name: 0, Length: 3538, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2369, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.23686976730823517, Val Loss: 0.22683587670326233\n",
      "model_1训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2268, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.22683587670326233, Val Loss: 0.21971289813518524\n",
      "model_1训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2197, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.21971289813518524, Val Loss: 0.22126643359661102\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2213, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.22126643359661102, Val Loss: 0.22014713287353516\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2201, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.22014713287353516, Val Loss: 0.21679337322711945\n",
      "model_1训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.21679337322711945, Val Loss: 0.21481694281101227\n",
      "model_1训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2148, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.21481694281101227, Val Loss: 0.2144109308719635\n",
      "model_1训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.2144, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.2144109308719635, Val Loss: 0.21275033056735992\n",
      "model_1训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.2128, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.21275033056735992, Val Loss: 0.20997272431850433\n",
      "model_1训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.2100, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.20997272431850433, Val Loss: 0.2077609896659851\n",
      "model_1训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.2078, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.2077609896659851, Val Loss: 0.20563998818397522\n",
      "model_1训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.2056, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.20563998818397522, Val Loss: 0.20246002078056335\n",
      "model_1训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.2025, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.20246002078056335, Val Loss: 0.19875314831733704\n",
      "model_1训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1988, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.19875314831733704, Val Loss: 0.19553335011005402\n",
      "model_1训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1955, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.19553335011005402, Val Loss: 0.19090288877487183\n",
      "model_1训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1909, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.19090288877487183, Val Loss: 0.18625687062740326\n",
      "model_1训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1863, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.18625687062740326, Val Loss: 0.18222163617610931\n",
      "model_1训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.1822, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.18222163617610931, Val Loss: 0.17769639194011688\n",
      "model_1训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.1777, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.17769639194011688, Val Loss: 0.173379585146904\n",
      "model_1训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.1734, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.173379585146904, Val Loss: 0.16901403665542603\n",
      "model_1训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.1690, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.16901403665542603, Val Loss: 0.16536132991313934\n",
      "model_1训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.1654, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.16536132991313934, Val Loss: 0.16167235374450684\n",
      "model_1训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.1617, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.16167235374450684, Val Loss: 0.1581098884344101\n",
      "model_1训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.1581, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.1581098884344101, Val Loss: 0.15473467111587524\n",
      "model_1训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.1547, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.15473467111587524, Val Loss: 0.15123678743839264\n",
      "model_1训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.1512, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.15123678743839264, Val Loss: 0.14776752889156342\n",
      "model_1训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.1478, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.14776752889156342, Val Loss: 0.14449413120746613\n",
      "model_1训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.1445, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.14449413120746613, Val Loss: 0.14156830310821533\n",
      "model_1训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.1416, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.14156830310821533, Val Loss: 0.13914461433887482\n",
      "model_1训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.1391, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.13914461433887482, Val Loss: 0.13786748051643372\n",
      "model_1训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.1379, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.13786748051643372, Val Loss: 0.14458782970905304\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.1446, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.14458782970905304, Val Loss: 0.1445796638727188\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.1446, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.1445796638727188, Val Loss: 0.1331549882888794\n",
      "model_1训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.1332, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.1331549882888794, Val Loss: 0.13942287862300873\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.1394, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.13942287862300873, Val Loss: 0.13118654489517212\n",
      "model_1训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.1312, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.13118654489517212, Val Loss: 0.13561522960662842\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.1356, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.13561522960662842, Val Loss: 0.12999489903450012\n",
      "model_1训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.1300, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.12999489903450012, Val Loss: 0.13109458982944489\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.1311, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.13109458982944489, Val Loss: 0.12737241387367249\n",
      "model_1训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.1274, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.12737241387367249, Val Loss: 0.12695343792438507\n",
      "model_1训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.1270, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.12695343792438507, Val Loss: 0.12548735737800598\n",
      "model_1训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.1255, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.12548735737800598, Val Loss: 0.12322171777486801\n",
      "model_1训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.1232, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.12322171777486801, Val Loss: 0.12350433319807053\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.1235, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.12350433319807053, Val Loss: 0.11935093998908997\n",
      "model_1训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.1194, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.11935093998908997, Val Loss: 0.12088633328676224\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.1209, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.12088633328676224, Val Loss: 0.11660595238208771\n",
      "model_1训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.1166, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.11660595238208771, Val Loss: 0.11751899123191833\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.1175, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.11751899123191833, Val Loss: 0.1152118667960167\n",
      "model_1训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.1152, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.1152118667960167, Val Loss: 0.1126563772559166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.1127, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.1126563772559166, Val Loss: 0.11372201889753342\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.1137, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.11372201889753342, Val Loss: 0.11062700301408768\n",
      "model_1训练结果更新为第50个模型\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.1106, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.11062700301408768, Val Loss: 0.1078716367483139\n",
      "model_1训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.1079, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.1078716367483139, Val Loss: 0.10907694697380066\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.1091, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.10907694697380066, Val Loss: 0.10843252390623093\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.1084, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.10843252390623093, Val Loss: 0.10326030105352402\n",
      "model_1训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.1033, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.10326030105352402, Val Loss: 0.1015942171216011\n",
      "model_1训练结果更新为第55个模型\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.1016, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.1015942171216011, Val Loss: 0.10350586473941803\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.1035, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.10350586473941803, Val Loss: 0.10422155261039734\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.1042, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.10422155261039734, Val Loss: 0.10459504276514053\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.1046, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.10459504276514053, Val Loss: 0.09640772640705109\n",
      "model_1训练结果更新为第59个模型\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0964, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.09640772640705109, Val Loss: 0.0921332985162735\n",
      "model_1训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0921, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.0921332985162735, Val Loss: 0.09282135963439941\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0928, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.09282135963439941, Val Loss: 0.09671629965305328\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0967, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.09671629965305328, Val Loss: 0.10590608417987823\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.1059, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.10590608417987823, Val Loss: 0.0920511782169342\n",
      "model_1训练结果更新为第64个模型\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0921, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.0920511782169342, Val Loss: 0.08346190303564072\n",
      "model_1训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0835, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.08346190303564072, Val Loss: 0.08500668406486511\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0850, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.08500668406486511, Val Loss: 0.09029919654130936\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0903, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.09029919654130936, Val Loss: 0.09569936990737915\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0957, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.09569936990737915, Val Loss: 0.07974664866924286\n",
      "model_1训练结果更新为第69个模型\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0797, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.07974664866924286, Val Loss: 0.0763164758682251\n",
      "model_1训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0763, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.0763164758682251, Val Loss: 0.0853172019124031\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0853, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.0853172019124031, Val Loss: 0.0834982693195343\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0835, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.0834982693195343, Val Loss: 0.07758892327547073\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0776, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.07758892327547073, Val Loss: 0.06863290071487427\n",
      "model_1训练结果更新为第74个模型\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0686, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.06863290071487427, Val Loss: 0.07067535072565079\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0707, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.07067535072565079, Val Loss: 0.08013060688972473\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0801, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.08013060688972473, Val Loss: 0.07578705251216888\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0758, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.07578705251216888, Val Loss: 0.0700635313987732\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0701, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.0700635313987732, Val Loss: 0.06103457137942314\n",
      "model_1训练结果更新为第79个模型\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0610, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.06103457137942314, Val Loss: 0.060657698661088943\n",
      "model_1训练结果更新为第80个模型\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0607, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.060657698661088943, Val Loss: 0.06763815134763718\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0676, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.06763815134763718, Val Loss: 0.07088116556406021\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0709, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.07088116556406021, Val Loss: 0.07644759863615036\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0764, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.07644759863615036, Val Loss: 0.05875775218009949\n",
      "model_1训练结果更新为第84个模型\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0588, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.05875775218009949, Val Loss: 0.051371797919273376\n",
      "model_1训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.051371797919273376, Val Loss: 0.05562857165932655\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0556, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.05562857165932655, Val Loss: 0.06218833848834038\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0622, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.06218833848834038, Val Loss: 0.07109348475933075\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0711, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.07109348475933075, Val Loss: 0.0541125126183033\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0541, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.0541125126183033, Val Loss: 0.04486742988228798\n",
      "model_1训练结果更新为第90个模型\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0449, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.04486742988228798, Val Loss: 0.04591498523950577\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0459, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.04591498523950577, Val Loss: 0.051998067647218704\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0520, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.051998067647218704, Val Loss: 0.06279418617486954\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.06279418617486954, Val Loss: 0.05359763279557228\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0536, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.05359763279557228, Val Loss: 0.04535718634724617\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0454, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.04535718634724617, Val Loss: 0.03723959997296333\n",
      "model_1训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0372, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.03723959997296333, Val Loss: 0.03924821689724922\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0392, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.03924821689724922, Val Loss: 0.04750313609838486\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0475, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.04750313609838486, Val Loss: 0.047146812081336975\n",
      "         \n",
      "第 1 组模型\n",
      "AUC: 0.9935864723708002\n",
      "ACC: 0.9458552946813608\n",
      "F1: 0.9614202799590303\n",
      "Recall: 0.9964614295824487\n",
      "MCC: 0.8771139605177686\n",
      "    \n",
      "SMILES    C#CC1(O)CCC2C3C(CCC21C)C4=C(CC(=O)CC4)CC3C\n",
      "active                                             1\n",
      "0                                           0.038497\n",
      "1                                                0.0\n",
      "2                                           0.106933\n",
      "                             ...                    \n",
      "1439                                             0.0\n",
      "1440                                        0.027321\n",
      "1441                                         0.34507\n",
      "1442                                        0.615567\n",
      "1443                                        0.348718\n",
      "Name: 0, Length: 3538, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练第0组epochs\n",
      "loss tensor(0.2442, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2442186176776886, Val Loss: 0.231735497713089\n",
      "model_2训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2317, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.231735497713089, Val Loss: 0.22119605541229248\n",
      "model_2训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2212, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.22119605541229248, Val Loss: 0.21745924651622772\n",
      "model_2训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2175, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.21745924651622772, Val Loss: 0.21670976281166077\n",
      "model_2训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2167, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.21670976281166077, Val Loss: 0.21118268370628357\n",
      "model_2训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2112, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.21118268370628357, Val Loss: 0.20410610735416412\n",
      "model_2训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2041, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.20410610735416412, Val Loss: 0.1989438235759735\n",
      "model_2训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1989, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.1989438235759735, Val Loss: 0.1917717605829239\n",
      "model_2训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1918, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.1917717605829239, Val Loss: 0.18183374404907227\n",
      "model_2训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.1818, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.18183374404907227, Val Loss: 0.17324590682983398\n",
      "model_2训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.1732, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.17324590682983398, Val Loss: 0.16310416162014008\n",
      "model_2训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.1631, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.16310416162014008, Val Loss: 0.15154479444026947\n",
      "model_2训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.1515, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.15154479444026947, Val Loss: 0.13886937499046326\n",
      "model_2训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1389, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.13886937499046326, Val Loss: 0.1255890280008316\n",
      "model_2训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1256, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.1255890280008316, Val Loss: 0.11235306411981583\n",
      "model_2训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1124, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.11235306411981583, Val Loss: 0.09818845987319946\n",
      "model_2训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0982, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.09818845987319946, Val Loss: 0.08444777131080627\n",
      "model_2训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0844, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.08444777131080627, Val Loss: 0.07145527750253677\n",
      "model_2训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0715, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.07145527750253677, Val Loss: 0.05926185101270676\n",
      "model_2训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0593, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.05926185101270676, Val Loss: 0.04890694096684456\n",
      "model_2训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0489, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.04890694096684456, Val Loss: 0.040384817868471146\n",
      "model_2训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0404, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.040384817868471146, Val Loss: 0.033350564539432526\n",
      "model_2训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0334, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.033350564539432526, Val Loss: 0.02794240042567253\n",
      "model_2训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0279, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.02794240042567253, Val Loss: 0.024042434990406036\n",
      "model_2训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0240, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.024042434990406036, Val Loss: 0.021126553416252136\n",
      "model_2训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0211, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.021126553416252136, Val Loss: 0.01881229132413864\n",
      "model_2训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0188, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.01881229132413864, Val Loss: 0.017149467021226883\n",
      "model_2训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.017149467021226883, Val Loss: 0.01597551442682743\n",
      "model_2训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0160, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.01597551442682743, Val Loss: 0.014882155694067478\n",
      "model_2训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.014882155694067478, Val Loss: 0.013748209923505783\n",
      "model_2训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.013748209923505783, Val Loss: 0.012869492173194885\n",
      "model_2训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.012869492173194885, Val Loss: 0.012037060223519802\n",
      "model_2训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0120, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.012037060223519802, Val Loss: 0.011088087223470211\n",
      "model_2训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.011088087223470211, Val Loss: 0.010287931188941002\n",
      "model_2训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.010287931188941002, Val Loss: 0.00965395662933588\n",
      "model_2训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.00965395662933588, Val Loss: 0.008984781801700592\n",
      "model_2训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.008984781801700592, Val Loss: 0.008380715735256672\n",
      "model_2训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.008380715735256672, Val Loss: 0.007953967899084091\n",
      "model_2训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.007953967899084091, Val Loss: 0.007555394899100065\n",
      "model_2训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.007555394899100065, Val Loss: 0.007137896493077278\n",
      "model_2训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.007137896493077278, Val Loss: 0.006803655996918678\n",
      "model_2训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.006803655996918678, Val Loss: 0.0065342397429049015\n",
      "model_2训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.0065342397429049015, Val Loss: 0.006238617934286594\n",
      "model_2训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.006238617934286594, Val Loss: 0.00592404929921031\n",
      "model_2训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.00592404929921031, Val Loss: 0.005654348526149988\n",
      "model_2训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.005654348526149988, Val Loss: 0.005442059598863125\n",
      "model_2训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.005442059598863125, Val Loss: 0.0052831838838756084\n",
      "model_2训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.0052831838838756084, Val Loss: 0.005198247265070677\n",
      "model_2训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Train Loss: 0.005198247265070677, Val Loss: 0.005264970473945141\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.005264970473945141, Val Loss: 0.005803239531815052\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.005803239531815052, Val Loss: 0.007030915468931198\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.007030915468931198, Val Loss: 0.009289269335567951\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.009289269335567951, Val Loss: 0.006910574622452259\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.006910574622452259, Val Loss: 0.004410160705447197\n",
      "model_2训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.004410160705447197, Val Loss: 0.006430512294173241\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.006430512294173241, Val Loss: 0.005120411980897188\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.005120411980897188, Val Loss: 0.004671248607337475\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.004671248607337475, Val Loss: 0.005615470930933952\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.005615470930933952, Val Loss: 0.004134116694331169\n",
      "model_2训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.004134116694331169, Val Loss: 0.005238152574747801\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.005238152574747801, Val Loss: 0.004544568248093128\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.004544568248093128, Val Loss: 0.0044148704037070274\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.0044148704037070274, Val Loss: 0.004857669118791819\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.004857669118791819, Val Loss: 0.0039302692748606205\n",
      "model_2训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.0039302692748606205, Val Loss: 0.004647971596568823\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.004647971596568823, Val Loss: 0.004066247958689928\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.004066247958689928, Val Loss: 0.0040894970297813416\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.0040894970297813416, Val Loss: 0.004336543846875429\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.004336543846875429, Val Loss: 0.003719113999977708\n",
      "model_2训练结果更新为第68个模型\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.003719113999977708, Val Loss: 0.004144055303186178\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.004144055303186178, Val Loss: 0.003925355616956949\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.003925355616956949, Val Loss: 0.0036309638526290655\n",
      "model_2训练结果更新为第71个模型\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.0036309638526290655, Val Loss: 0.0039713201113045216\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.0039713201113045216, Val Loss: 0.003650768892839551\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.003650768892839551, Val Loss: 0.0034868563525378704\n",
      "model_2训练结果更新为第74个模型\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.0034868563525378704, Val Loss: 0.0037217566277831793\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.0037217566277831793, Val Loss: 0.0034912650007754564\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.0034912650007754564, Val Loss: 0.0032641736324876547\n",
      "model_2训练结果更新为第77个模型\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.0032641736324876547, Val Loss: 0.003400924615561962\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.003400924615561962, Val Loss: 0.003410238539800048\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.003410238539800048, Val Loss: 0.003191546304151416\n",
      "model_2训练结果更新为第80个模型\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.003191546304151416, Val Loss: 0.0030485657043755054\n",
      "model_2训练结果更新为第81个模型\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.0030485657043755054, Val Loss: 0.0031166276894509792\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.0031166276894509792, Val Loss: 0.0031962930224835873\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.0031962930224835873, Val Loss: 0.0031326604075729847\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.0031326604075729847, Val Loss: 0.002986637642607093\n",
      "model_2训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.002986637642607093, Val Loss: 0.0028401860035955906\n",
      "model_2训练结果更新为第86个模型\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.0028401860035955906, Val Loss: 0.002767025027424097\n",
      "model_2训练结果更新为第87个模型\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.002767025027424097, Val Loss: 0.002767023164778948\n",
      "model_2训练结果更新为第88个模型\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.002767023164778948, Val Loss: 0.00281603098846972\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.00281603098846972, Val Loss: 0.0029225386679172516\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.0029225386679172516, Val Loss: 0.003138901898637414\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.003138901898637414, Val Loss: 0.003725383197888732\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.003725383197888732, Val Loss: 0.005527782719582319\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.005527782719582319, Val Loss: 0.014565275982022285\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.014565275982022285, Val Loss: 0.062547467648983\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0625, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.062547467648983, Val Loss: 0.019785217940807343\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0198, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.019785217940807343, Val Loss: 0.01214085053652525\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.01214085053652525, Val Loss: 0.015025178901851177\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.015025178901851177, Val Loss: 0.009739350527524948\n",
      "         \n",
      "第 2 组模型\n",
      "AUC: 0.99992439849553\n",
      "ACC: 0.9770004791566842\n",
      "F1: 0.9832985386221295\n",
      "Recall: 1.0\n",
      "MCC: 0.9477705096707632\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES    C1=C(C2=C(N=C1N3CCN(CC)CC3)CCCCCC2)C4=CC=C(F)C=C4\n",
      "active                                                    1\n",
      "0                                                   0.15136\n",
      "1                                                       0.0\n",
      "2                                                  0.089399\n",
      "                                ...                        \n",
      "1439                                                0.31617\n",
      "1440                                               0.047363\n",
      "1441                                               0.323944\n",
      "1442                                               0.627516\n",
      "1443                                               0.364103\n",
      "Name: 0, Length: 3538, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2558, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2558170557022095, Val Loss: 0.2452452927827835\n",
      "model_3训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2452, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.2452452927827835, Val Loss: 0.22780857980251312\n",
      "model_3训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2278, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.22780857980251312, Val Loss: 0.21893829107284546\n",
      "model_3训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2189, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.21893829107284546, Val Loss: 0.22120912373065948\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2212, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.22120912373065948, Val Loss: 0.22227692604064941\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2223, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.22227692604064941, Val Loss: 0.21879521012306213\n",
      "model_3训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2188, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.21879521012306213, Val Loss: 0.21401432156562805\n",
      "model_3训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.2140, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.21401432156562805, Val Loss: 0.2128092497587204\n",
      "model_3训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.2128, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.2128092497587204, Val Loss: 0.21238653361797333\n",
      "model_3训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.2124, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.21238653361797333, Val Loss: 0.2108059674501419\n",
      "model_3训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.2108, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.2108059674501419, Val Loss: 0.20713557302951813\n",
      "model_3训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.2071, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.20713557302951813, Val Loss: 0.20312608778476715\n",
      "model_3训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.2031, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.20312608778476715, Val Loss: 0.200888991355896\n",
      "model_3训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.2009, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.200888991355896, Val Loss: 0.1981177181005478\n",
      "model_3训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1981, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.1981177181005478, Val Loss: 0.1939115822315216\n",
      "model_3训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1939, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.1939115822315216, Val Loss: 0.18910983204841614\n",
      "model_3训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1891, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.18910983204841614, Val Loss: 0.18414269387722015\n",
      "model_3训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.1841, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.18414269387722015, Val Loss: 0.1790696084499359\n",
      "model_3训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.1791, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.1790696084499359, Val Loss: 0.1728566288948059\n",
      "model_3训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.1729, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.1728566288948059, Val Loss: 0.16721133887767792\n",
      "model_3训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.1672, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.16721133887767792, Val Loss: 0.1618669033050537\n",
      "model_3训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.1619, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.1618669033050537, Val Loss: 0.15668587386608124\n",
      "model_3训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.1567, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.15668587386608124, Val Loss: 0.15278123319149017\n",
      "model_3训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.1528, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.15278123319149017, Val Loss: 0.14858970046043396\n",
      "model_3训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.1486, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.14858970046043396, Val Loss: 0.1457361876964569\n",
      "model_3训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.1457, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.1457361876964569, Val Loss: 0.14271539449691772\n",
      "model_3训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.1427, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.14271539449691772, Val Loss: 0.1408746987581253\n",
      "model_3训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.1409, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.1408746987581253, Val Loss: 0.1387694925069809\n",
      "model_3训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.1388, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.1387694925069809, Val Loss: 0.137283593416214\n",
      "model_3训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.1373, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.137283593416214, Val Loss: 0.1358226090669632\n",
      "model_3训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.1358, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.1358226090669632, Val Loss: 0.13411873579025269\n",
      "model_3训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.1341, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.13411873579025269, Val Loss: 0.13287337124347687\n",
      "model_3训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.1329, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.13287337124347687, Val Loss: 0.13146768510341644\n",
      "model_3训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.1315, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.13146768510341644, Val Loss: 0.12982012331485748\n",
      "model_3训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.1298, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.12982012331485748, Val Loss: 0.1281888782978058\n",
      "model_3训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.1282, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.1281888782978058, Val Loss: 0.1266317218542099\n",
      "model_3训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.1266, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.1266317218542099, Val Loss: 0.1251683533191681\n",
      "model_3训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.1252, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.1251683533191681, Val Loss: 0.12463017553091049\n",
      "model_3训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.1246, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.12463017553091049, Val Loss: 0.1274845153093338\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.1275, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.1274845153093338, Val Loss: 0.12448624521493912\n",
      "model_3训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.1245, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.12448624521493912, Val Loss: 0.11796212941408157\n",
      "model_3训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.1180, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.11796212941408157, Val Loss: 0.11891227960586548\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.1189, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.11891227960586548, Val Loss: 0.11802133917808533\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.1180, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.11802133917808533, Val Loss: 0.1130458191037178\n",
      "model_3训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.1130, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.1130458191037178, Val Loss: 0.11250516772270203\n",
      "model_3训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.1125, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.11250516772270203, Val Loss: 0.11294358223676682\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.1129, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.11294358223676682, Val Loss: 0.10980215668678284\n",
      "model_3训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.1098, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.10980215668678284, Val Loss: 0.10635360330343246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_3训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.1064, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.10635360330343246, Val Loss: 0.10717234760522842\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.1072, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.10717234760522842, Val Loss: 0.10874257981777191\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.1087, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.10874257981777191, Val Loss: 0.10373528301715851\n",
      "model_3训练结果更新为第50个模型\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.1037, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.10373528301715851, Val Loss: 0.09944802522659302\n",
      "model_3训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0994, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.09944802522659302, Val Loss: 0.0985875129699707\n",
      "model_3训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0986, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.0985875129699707, Val Loss: 0.10042066872119904\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.1004, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.10042066872119904, Val Loss: 0.10581377893686295\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.1058, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.10581377893686295, Val Loss: 0.09882166981697083\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0988, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.09882166981697083, Val Loss: 0.09139236062765121\n",
      "model_3训练结果更新为第56个模型\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0914, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.09139236062765121, Val Loss: 0.08933756500482559\n",
      "model_3训练结果更新为第57个模型\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0893, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.08933756500482559, Val Loss: 0.09307432919740677\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0931, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.09307432919740677, Val Loss: 0.09863239526748657\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0986, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.09863239526748657, Val Loss: 0.08758754283189774\n",
      "model_3训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0876, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.08758754283189774, Val Loss: 0.08170568197965622\n",
      "model_3训练结果更新为第61个模型\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0817, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.08170568197965622, Val Loss: 0.08482874929904938\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0848, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.08482874929904938, Val Loss: 0.08726410567760468\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0873, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.08726410567760468, Val Loss: 0.08620025962591171\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0862, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.08620025962591171, Val Loss: 0.0758747011423111\n",
      "model_3训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0759, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.0758747011423111, Val Loss: 0.07491213828325272\n",
      "model_3训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0749, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.07491213828325272, Val Loss: 0.08158212900161743\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0816, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.08158212900161743, Val Loss: 0.07932159304618835\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0793, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.07932159304618835, Val Loss: 0.07357320189476013\n",
      "model_3训练结果更新为第69个模型\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0736, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.07357320189476013, Val Loss: 0.06643467396497726\n",
      "model_3训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0664, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.06643467396497726, Val Loss: 0.06700289249420166\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0670, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.06700289249420166, Val Loss: 0.07358372956514359\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0736, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.07358372956514359, Val Loss: 0.0743950605392456\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0744, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.0743950605392456, Val Loss: 0.07387419044971466\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0739, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.07387419044971466, Val Loss: 0.06082288920879364\n",
      "model_3训练结果更新为第75个模型\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0608, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.06082288920879364, Val Loss: 0.05668989196419716\n",
      "model_3训练结果更新为第76个模型\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0567, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.05668989196419716, Val Loss: 0.06183692812919617\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0618, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.06183692812919617, Val Loss: 0.06714572757482529\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0671, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.06714572757482529, Val Loss: 0.07319092750549316\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0732, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.07319092750549316, Val Loss: 0.05646258220076561\n",
      "model_3训练结果更新为第80个模型\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0565, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.05646258220076561, Val Loss: 0.04882260411977768\n",
      "model_3训练结果更新为第81个模型\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0488, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.04882260411977768, Val Loss: 0.05334850028157234\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0533, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.05334850028157234, Val Loss: 0.05893462151288986\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0589, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.05893462151288986, Val Loss: 0.06283728778362274\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.06283728778362274, Val Loss: 0.04844725877046585\n",
      "model_3训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0484, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.04844725877046585, Val Loss: 0.04215798154473305\n",
      "model_3训练结果更新为第86个模型\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0422, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.04215798154473305, Val Loss: 0.04655494913458824\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0466, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.04655494913458824, Val Loss: 0.051377806812524796\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.051377806812524796, Val Loss: 0.0551237016916275\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0551, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.0551237016916275, Val Loss: 0.043345753103494644\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0433, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.043345753103494644, Val Loss: 0.03609297052025795\n",
      "model_3训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0361, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.03609297052025795, Val Loss: 0.036856330931186676\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0369, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.036856330931186676, Val Loss: 0.04170757532119751\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0417, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.04170757532119751, Val Loss: 0.048992037773132324\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0490, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.048992037773132324, Val Loss: 0.045153722167015076\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0452, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.045153722167015076, Val Loss: 0.04152311012148857\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0415, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.04152311012148857, Val Loss: 0.031591035425662994\n",
      "model_3训练结果更新为第97个模型\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0316, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.031591035425662994, Val Loss: 0.028485506772994995\n",
      "model_3训练结果更新为第98个模型\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0285, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.028485506772994995, Val Loss: 0.032226577401161194\n",
      "         \n",
      "第 3 组模型\n",
      "AUC: 0.9953565470785734\n",
      "ACC: 0.981312889314806\n",
      "F1: 0.9862917398945519\n",
      "Recall: 0.9880281690140845\n",
      "MCC: 0.9569612299134599\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES    CC(=O)CC(C)C\n",
      "active               1\n",
      "0                  0.0\n",
      "1             0.008262\n",
      "2             0.006112\n",
      "              ...     \n",
      "1439               0.0\n",
      "1440          0.001266\n",
      "1441          0.028571\n",
      "1442          0.438565\n",
      "1443          0.066667\n",
      "Name: 0, Length: 3538, dtype: object\n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2386, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2385978102684021, Val Loss: 0.22959575057029724\n",
      "model_4训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2296, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.22959575057029724, Val Loss: 0.22051414847373962\n",
      "model_4训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2205, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.22051414847373962, Val Loss: 0.2199510782957077\n",
      "model_4训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2200, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.2199510782957077, Val Loss: 0.22099708020687103\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2210, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.22099708020687103, Val Loss: 0.21855439245700836\n",
      "model_4训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2186, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.21855439245700836, Val Loss: 0.21683570742607117\n",
      "model_4训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.21683570742607117, Val Loss: 0.21636109054088593\n",
      "model_4训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.2164, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.21636109054088593, Val Loss: 0.21594545245170593\n",
      "model_4训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.2159, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.21594545245170593, Val Loss: 0.21422217786312103\n",
      "model_4训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.2142, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.21422217786312103, Val Loss: 0.21231119334697723\n",
      "model_4训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.2123, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.21231119334697723, Val Loss: 0.2108558863401413\n",
      "model_4训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.2109, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.2108558863401413, Val Loss: 0.20906181633472443\n",
      "model_4训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.2091, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.20906181633472443, Val Loss: 0.2064950168132782\n",
      "model_4训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.2065, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.2064950168132782, Val Loss: 0.20381374657154083\n",
      "model_4训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.2038, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.20381374657154083, Val Loss: 0.20094038546085358\n",
      "model_4训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.2009, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.20094038546085358, Val Loss: 0.1971740573644638\n",
      "model_4训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1972, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.1971740573644638, Val Loss: 0.19350801408290863\n",
      "model_4训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.1935, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.19350801408290863, Val Loss: 0.1894201934337616\n",
      "model_4训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.1894, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.1894201934337616, Val Loss: 0.1854952722787857\n",
      "model_4训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.1855, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.1854952722787857, Val Loss: 0.18143822252750397\n",
      "model_4训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.1814, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.18143822252750397, Val Loss: 0.17802837491035461\n",
      "model_4训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.1780, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.17802837491035461, Val Loss: 0.17459075152873993\n",
      "model_4训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.1746, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.17459075152873993, Val Loss: 0.17166057229042053\n",
      "model_4训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.1717, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.17166057229042053, Val Loss: 0.16938109695911407\n",
      "model_4训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.1694, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.16938109695911407, Val Loss: 0.16797378659248352\n",
      "model_4训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.1680, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.16797378659248352, Val Loss: 0.16850334405899048\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.1685, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.16850334405899048, Val Loss: 0.1640566885471344\n",
      "model_4训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.1641, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.1640566885471344, Val Loss: 0.16114316880702972\n",
      "model_4训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.1611, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.16114316880702972, Val Loss: 0.1612425148487091\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.1612, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.1612425148487091, Val Loss: 0.1570008248090744\n",
      "model_4训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.1570, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.1570008248090744, Val Loss: 0.15378518402576447\n",
      "model_4训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.1538, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.15378518402576447, Val Loss: 0.1529679000377655\n",
      "model_4训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.1530, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.1529679000377655, Val Loss: 0.14948895573616028\n",
      "model_4训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.1495, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.14948895573616028, Val Loss: 0.14530779421329498\n",
      "model_4训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.1453, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.14530779421329498, Val Loss: 0.1425684243440628\n",
      "model_4训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.1426, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.1425684243440628, Val Loss: 0.14160363376140594\n",
      "model_4训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.1416, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.14160363376140594, Val Loss: 0.14435285329818726\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.1444, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.14435285329818726, Val Loss: 0.13971957564353943\n",
      "model_4训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.1397, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.13971957564353943, Val Loss: 0.13247832655906677\n",
      "model_4训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.1325, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.13247832655906677, Val Loss: 0.12862235307693481\n",
      "model_4训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.1286, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.12862235307693481, Val Loss: 0.13137491047382355\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.1314, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.13137491047382355, Val Loss: 0.1355174332857132\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.1355, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.1355174332857132, Val Loss: 0.12182123959064484\n",
      "model_4训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.1218, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.12182123959064484, Val Loss: 0.1226036548614502\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.1226, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.1226036548614502, Val Loss: 0.13104692101478577\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.1310, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.13104692101478577, Val Loss: 0.11560149490833282\n",
      "model_4训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.1156, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.11560149490833282, Val Loss: 0.11825741827487946\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.1183, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.11825741827487946, Val Loss: 0.12734130024909973\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.1273, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.12734130024909973, Val Loss: 0.10886628925800323\n",
      "model_4训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.1089, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.10886628925800323, Val Loss: 0.12133769690990448\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.1213, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.12133769690990448, Val Loss: 0.12531302869319916\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.1253, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.12531302869319916, Val Loss: 0.10645665973424911\n",
      "model_4训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.1065, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Train Loss: 0.10645665973424911, Val Loss: 0.13498011231422424\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.1350, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.13498011231422424, Val Loss: 0.10917175561189651\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.1092, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.10917175561189651, Val Loss: 0.11607548594474792\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.1161, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.11607548594474792, Val Loss: 0.10641437768936157\n",
      "model_4训练结果更新为第55个模型\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.1064, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.10641437768936157, Val Loss: 0.10172152519226074\n",
      "model_4训练结果更新为第56个模型\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.1017, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.10172152519226074, Val Loss: 0.1082935556769371\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.1083, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.1082935556769371, Val Loss: 0.09631050378084183\n",
      "model_4训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0963, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.09631050378084183, Val Loss: 0.10955595970153809\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.1096, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.10955595970153809, Val Loss: 0.09202559292316437\n",
      "model_4训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0920, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.09202559292316437, Val Loss: 0.09958347678184509\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0996, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.09958347678184509, Val Loss: 0.09143562614917755\n",
      "model_4训练结果更新为第62个模型\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0914, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.09143562614917755, Val Loss: 0.08965788781642914\n",
      "model_4训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0897, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.08965788781642914, Val Loss: 0.09350447356700897\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0935, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.09350447356700897, Val Loss: 0.08356674015522003\n",
      "model_4训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0836, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.08356674015522003, Val Loss: 0.09174635261297226\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0917, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.09174635261297226, Val Loss: 0.08844981342554092\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0884, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.08844981342554092, Val Loss: 0.07966546714305878\n",
      "model_4训练结果更新为第68个模型\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0797, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.07966546714305878, Val Loss: 0.09092791378498077\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0909, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.09092791378498077, Val Loss: 0.08689850568771362\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0869, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.08689850568771362, Val Loss: 0.07484596222639084\n",
      "model_4训练结果更新为第71个模型\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0748, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.07484596222639084, Val Loss: 0.09124885499477386\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0912, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.09124885499477386, Val Loss: 0.09058351814746857\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0906, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.09058351814746857, Val Loss: 0.07061611115932465\n",
      "model_4训练结果更新为第74个模型\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0706, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.07061611115932465, Val Loss: 0.09915738552808762\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0992, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.09915738552808762, Val Loss: 0.0958445817232132\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0958, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.0958445817232132, Val Loss: 0.07239686697721481\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0724, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.07239686697721481, Val Loss: 0.11636802554130554\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.1164, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.11636802554130554, Val Loss: 0.08116643875837326\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0812, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.08116643875837326, Val Loss: 0.08500035852193832\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0850, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.08500035852193832, Val Loss: 0.08625532686710358\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0863, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.08625532686710358, Val Loss: 0.06126774847507477\n",
      "model_4训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0613, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.06126774847507477, Val Loss: 0.08044266700744629\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0804, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.08044266700744629, Val Loss: 0.05854150280356407\n",
      "model_4训练结果更新为第84个模型\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0585, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.05854150280356407, Val Loss: 0.07870865613222122\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0787, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.07870865613222122, Val Loss: 0.06500250101089478\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0650, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.06500250101089478, Val Loss: 0.06512662023305893\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0651, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.06512662023305893, Val Loss: 0.06814786791801453\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0681, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.06814786791801453, Val Loss: 0.05325663089752197\n",
      "model_4训练结果更新为第89个模型\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0533, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.05325663089752197, Val Loss: 0.0637635663151741\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0638, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.0637635663151741, Val Loss: 0.051731906831264496\n",
      "model_4训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0517, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.051731906831264496, Val Loss: 0.05683249235153198\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0568, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.05683249235153198, Val Loss: 0.05706999450922012\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0571, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.05706999450922012, Val Loss: 0.0480000302195549\n",
      "model_4训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0480, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.0480000302195549, Val Loss: 0.05788762867450714\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0579, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.05788762867450714, Val Loss: 0.05140545964241028\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.05140545964241028, Val Loss: 0.04535743221640587\n",
      "model_4训练结果更新为第97个模型\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0454, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.04535743221640587, Val Loss: 0.05501704290509224\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0550, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.05501704290509224, Val Loss: 0.04938259720802307\n",
      "         \n",
      "第 4 组模型\n",
      "AUC: 0.9834739447826417\n",
      "ACC: 0.9487302347867753\n",
      "F1: 0.9615522817103845\n",
      "Recall: 0.9449152542372882\n",
      "MCC: 0.8859365316554197\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "for i in range(start_index, end_index+1):\n",
    "    \n",
    "    # 读取数据\n",
    "    file_path = file_prefix + str(i) + file_extension\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    first_row = data.iloc[0]\n",
    "    print(first_row)\n",
    "    \n",
    "    # 提取特征和标签\n",
    "    x_input = data.drop(['SMILES'], axis=1).values\n",
    "    y_output = data['active'].values\n",
    "    \n",
    "        # 遍历数据，将 inf 赋值为 0\n",
    "    x_input = np.nan_to_num(x_input, posinf=0, neginf=0)\n",
    "    \n",
    "#        # 检查并输出非 float64 数据的坐标\n",
    "#    for row_index, row in enumerate(x_input):\n",
    "#        for col_index, value in enumerate(row):\n",
    "#            if x_input.dtype!= np.float64:\n",
    "#                print(f\"非 float64 数据位于 ({row_index}, {col_index})，值为: {value}\")\n",
    "#    \n",
    "    #print('x_input',x_input)\n",
    "    #print('y_output',y_output)\n",
    "    \n",
    "    \n",
    "    val_x_input = data.drop(['SMILES'], axis=1).values\n",
    "    val_y_output = data['active'].values\n",
    "    \n",
    "    val_x_input = np.nan_to_num(x_input, posinf=0, neginf=0)\n",
    "    \n",
    "#    # 查找 x_input 中的无穷值位置\n",
    "#    infinity_positions_x = np.argwhere(np.isinf(x_input))\n",
    "#    if len(infinity_positions_x) > 0:\n",
    "#        column_names = data.drop(['SMILES'], axis=1).columns\n",
    "#        infinity_column_names_x = [column_names[pos[1]] for pos in infinity_positions_x]\n",
    "#        print(\"在 x_input 中的无穷值所在列名：\", infinity_column_names_x, '         位置：',infinity_positions_x)\n",
    "#    else:\n",
    "#        print(\"在 x_input 中没有无穷值\")\n",
    "#\n",
    "#    # 查找 val_x_input 中的无穷值位置\n",
    "#    infinity_positions_val_x = np.argwhere(np.isinf(val_x_input))\n",
    "#    if len(infinity_positions_val_x) > 0:\n",
    "#        column_names = data.drop(['SMILES'], axis=1).columns\n",
    "#        infinity_column_names_val_x = [column_names[pos[1]] for pos in infinity_positions_val_x]\n",
    "#        print(\"在 val_x_input 中的无穷值所在列名：\", infinity_column_names_val_x, '        位置：',infinity_positions_x)\n",
    "#    else:\n",
    "#        print(\"在 val_x_input 中没有无穷值\")\n",
    "    \n",
    "        # 标准化特征（假设特征是数值型的）\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    x_input = scaler.fit_transform(x_input)\n",
    "    val_x_input = scaler.fit_transform(val_x_input)\n",
    "\n",
    "    # 数据放缩处理（例如，将特征值缩放到 0 到 1 之间）\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_input = min_max_scaler.fit_transform(x_input)\n",
    "    val_x_input = min_max_scaler.fit_transform(val_x_input)\n",
    "    \n",
    "        # 转换为Tensor\n",
    "    x = torch.Tensor(x_input)\n",
    "    y_true = torch.Tensor(y_output).view(-1, 1)    \n",
    "    \n",
    "    val_x = torch.Tensor(val_x_input)\n",
    "    val_y_true = torch.Tensor(val_y_output).view(-1, 1)\n",
    "    \n",
    "    \n",
    "    # 数据维度\n",
    "    input_dim = x.shape[1]\n",
    "    output_dim = 1\n",
    "    hidden_dim1 = 256\n",
    "    hidden_dim2 = 128\n",
    "    hidden_dim3 = 32\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    weight_decay = 0.001\n",
    "    # MLP模型定义\n",
    "    mlp = nn.Sequential(  \n",
    "        nn.Linear(input_dim, hidden_dim1),  # 第一层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim1, hidden_dim2),  # 第二层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim2, hidden_dim3),  # 第三层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim3, output_dim),  # 输出层  \n",
    "        nn.Sigmoid()  \n",
    "    )  \n",
    "        \n",
    "    # 优化器和损失函数\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "    #loss_func =  nn.BCEWithLogitsLoss()\n",
    "    loss_func =  nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    num_iterations_without_improvement = 0  # Initialize counter for iterations without improvement. \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"开始训练第{epoch}组epochs\")\n",
    "        prediction = mlp(x)\n",
    "        #print('prediction',prediction)\n",
    "        \n",
    "        #prediction_np = prediction.detach().numpy()\n",
    "        #df = pd.DataFrame(prediction_np)\n",
    "        #df.to_csv('prediction.csv', index=False)\n",
    "        \n",
    "        loss = loss_func(prediction, y_true)\n",
    "        print('loss',loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(optimizer)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_prediction=mlp(val_x)\n",
    "            val_loss = loss_func(val_prediction, val_y_true)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "        \n",
    "        #print(\"检查数据和模型输出:\")\n",
    "        #print(\"是否存在 NaN 或无穷大值在训练预测结果中:\", torch.isnan(prediction).any() or torch.isinf(prediction).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在验证预测结果中:\", torch.isnan(val_prediction).any() or torch.isinf(val_prediction).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在训练真实值中:\", torch.isnan(y_true).any() or torch.isinf(y_true).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在验证真实值中:\", torch.isnan(val_y_true).any() or torch.isinf(val_y_true).any())\n",
    "        #\n",
    "          \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  \n",
    "            num_iterations_without_improvement = 0\n",
    "            torch.save(mlp.state_dict(), f\"models_{file}/model_{i+1}.pt\")\n",
    "            #torch.save(mlp, \"model_saved/mlp1.pt\")\n",
    "            print(f'model_{i}训练结果更新为第{epoch}个模型')\n",
    "            best_epoch = epoch\n",
    "        else :\n",
    "            num_iterations_without_improvement += 1  \n",
    "            if num_iterations_without_improvement == 300:\n",
    "                print('300次迭代没有更新，结束迭代')\n",
    "                print(f'最棒的模型是第{best_epoch}个epoch')\n",
    "                break\n",
    "        print('         ')\n",
    "        \n",
    "\n",
    "    prediction_np = prediction.data.numpy()\n",
    "        # 将 prediction 添加到 DataFrame\n",
    "    #data['prediction'] = prediction_np\n",
    "        # 将 DataFrame 保存到 CSV 文件\n",
    "    #data.to_csv('output/output_train.csv', index=True)\n",
    "    \n",
    "    # 预测并计算AUC\n",
    "    prediction_np = prediction.detach().numpy().flatten()\n",
    "    auc = roc_auc_score(y_true, prediction_np)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    binary_prediction = np.where(prediction_np > threshold, 1, 0)\n",
    "    \n",
    "    acc = accuracy_score(y_true, binary_prediction)\n",
    "    f1 = f1_score(y_true, binary_prediction)\n",
    "    recall = recall_score(y_true, binary_prediction)\n",
    "    mcc = matthews_corrcoef(y_true, binary_prediction)\n",
    "    output_file = \"train_scores.csv\"\n",
    "    \n",
    "    \n",
    "    auc_scores.append(auc)\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    recall_scores.append(recall)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    print('第',i,'组模型')\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"ACC:\", acc)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print('    ')\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972b13e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "AUC_1: 0.8424947145877377\n",
      "Accuracy_1: 0.7633587786259542\n",
      "f1_1: 0.8132530120481928\n",
      "recall_1: 0.7670454545454546\n",
      "mcc_1: 0.5002363589372892\n",
      "             \n",
      "             \n",
      "False\n",
      "False\n",
      "AUC_2: 0.895066735491483\n",
      "Accuracy_2: 0.7900763358778626\n",
      "f1_2: 0.8405797101449276\n",
      "recall_2: 0.7591623036649214\n",
      "mcc_2: 0.5710401914343242\n",
      "             \n",
      "             \n",
      "False\n",
      "False\n",
      "AUC_3: 0.7687041219649915\n",
      "Accuracy_3: 0.7279693486590039\n",
      "f1_3: 0.7977207977207976\n",
      "recall_3: 0.7608695652173914\n",
      "mcc_3: 0.3897168370610816\n",
      "             \n",
      "             \n",
      "False\n",
      "False\n",
      "AUC_4: 0.8927648578811369\n",
      "Accuracy_4: 0.816793893129771\n",
      "f1_4: 0.8545454545454545\n",
      "recall_4: 0.8197674418604651\n",
      "mcc_4: 0.6123306925907216\n",
      "             \n",
      "             \n",
      "Average AUC: 0.849758±0.051296\n",
      "Average Accuracy: 0.774550±0.032866\n",
      "Average F1: 0.826525±0.022296\n",
      "Average Recall: 0.776711±0.025031\n",
      "Average MCC: 0.518331±0.084385\n"
     ]
    }
   ],
   "source": [
    "te_file_prefix = f\"{file}_Feature_fusion/data_test\"\n",
    "te_file_extension = \".csv\"\n",
    "\n",
    "import os \n",
    "\n",
    "avg_auc = 0.0  \n",
    "avg_acc = 0.0  \n",
    "avg_f1 = 0.0  \n",
    "avg_recall = 0.0  \n",
    "avg_mcc = 0.0  \n",
    "\n",
    "\n",
    "#te_file_paths = [te_file_prefix + str(i) + te_file_extension for i in range(start_index, end_index+1)]  \n",
    "results = pd.DataFrame(columns=['AUC', 'Accuracy', 'F1', 'Recall', 'MCC'])   \n",
    "    \n",
    "#for i,te_file_path in enumerate(te_file_paths):  \n",
    "for i in range(start_index, end_index): \n",
    "    te_file_path = te_file_prefix + str(i) + te_file_extension\n",
    "    data_test = pd.read_csv(te_file_path)  \n",
    "    if file == 'BBB':\n",
    "        data_test = data_test.head(129)  \n",
    "\n",
    "    #print(data_test)\n",
    "      \n",
    "    te_x_input = data_test.drop(['SMILES'], axis=1).values  \n",
    "    #print('1te_x_input.shape',te_x_input.shape)\n",
    "    \n",
    "    te_y_true = data_test['active'].values  \n",
    "    te_x_input = te_x_input.astype(np.float64)\n",
    "    #print('2te_x_input.shape',te_x_input.shape)\n",
    "    \n",
    "    te_x_input = np.nan_to_num(te_x_input, posinf=0, neginf=0)\n",
    "    \n",
    "    \n",
    "    print(np.isnan(te_y_true).any())  # 检查 te_y_true 是否包含 NaN\n",
    "    print(np.isinf(te_y_true).any())  # 检查 te_y_true 是否包含无穷值\n",
    "    \n",
    "    #print('3te_x_input',te_x_input.shape)    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    te_x_input = scaler.fit_transform(te_x_input)\n",
    "\n",
    "    # 数据放缩处理（例如，将特征值缩放到 0 到 1 之间）\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    te_x_input = min_max_scaler.fit_transform(te_x_input)\n",
    "    te_x_input = torch.Tensor(te_x_input)\n",
    "    #print('4te_x_input.shape',te_x_input.shape)\n",
    "    mlp.load_state_dict(torch.load(f\"models_{file}/model_{i+1}.pt\")) \n",
    "    \n",
    "    #print('39 te_x_input te_x_input ',te_x_input)\n",
    "    \n",
    "    #print(\"Length of te_prediction:\", len(te_prediction))\n",
    "    #print(\"Length of data_test:\", len(data_test))\n",
    "    #print(\"Length of data index:\", len(data.index))\n",
    "    #\n",
    "    with torch.no_grad():    \n",
    "        \n",
    "       # print(\"47  te_x_input  te_x_input\",te_x_input)\n",
    "        \n",
    "        te_prediction = mlp(te_x_input)  \n",
    "        \n",
    "        ############################################################\n",
    "        te_prediction = te_prediction.numpy()\n",
    "        data = pd.DataFrame(te_prediction)\n",
    "        data.to_csv(f'output1/test_prediction{i+1}.csv', mode='a', index=False)\n",
    "        ############################################################\n",
    "        \n",
    "        #print('te_y_true',len(te_y_true))\n",
    "        #print('te_prediction',len(te_prediction))\n",
    "        #print('te_y_true',te_y_true)\n",
    "        #print('te_prediction',te_prediction)\n",
    "        #print('检查 te_y_true 是否包含 NaN',np.isnan(te_y_true).any())  # 检查 te_y_true 是否包含 NaN\n",
    "        #print('检查 te_y_true 是否包含无穷值',np.isinf(te_y_true).any())  # 检查 te_y_true 是否包含无穷值\n",
    "        #print('检查 te_prediction 是否包含 NaN',np.isnan(te_prediction).any())  # 检查 te_prediction 是否包含 NaN\n",
    "        #print('检查 te_prediction 是否包含无穷值',np.isinf(te_prediction).any())  # 检查 te_prediction 是否包含无穷值\n",
    "        #\n",
    "        auc = roc_auc_score(te_y_true, te_prediction)\n",
    "        #print(auc)\n",
    "        \n",
    "        #print('54  te_predictionte_prediction',te_prediction)    \n",
    "        #te_prediction = te_prediction.numpy()      \n",
    "        data_test[f'te_prediction_{i+1}'] = te_prediction \n",
    "        \n",
    "        auc = roc_auc_score(te_y_true, te_prediction)\n",
    "        #print(auc)\n",
    "        \n",
    "        #data_test.to_csv(f'output1/te_prediction{i+1}.csv', mode='a', index=True)\n",
    "        #print('te_prediction  ',te_prediction)\n",
    "        \n",
    "        auc = roc_auc_score(te_y_true, te_prediction)  \n",
    "        print(f'AUC_{i+1}: {auc}')\n",
    "        \n",
    "        te_prediction_binary = np.where(te_prediction > 0.5, 1, 0)  \n",
    "        acc = accuracy_score(te_y_true, te_prediction_binary)  \n",
    "        print(f'Accuracy_{i+1}: {acc}')\n",
    "        f1 = f1_score(te_y_true, te_prediction_binary)\n",
    "        recall = recall_score(te_y_true, te_prediction_binary)\n",
    "        mcc = matthews_corrcoef(te_y_true, te_prediction_binary)\n",
    "        print(f'f1_{i+1}: {f1}')\n",
    "        print(f'recall_{i+1}: {recall}')\n",
    "        print(f'mcc_{i+1}: {mcc}')\n",
    "        print('             ')\n",
    "        print('             ')\n",
    "        \n",
    "        avg_auc += auc  \n",
    "        avg_acc += acc  \n",
    "        avg_f1 += f1  \n",
    "        avg_recall += recall  \n",
    "        avg_mcc += mcc\n",
    "        \n",
    "        results = results.append({'AUC': auc, 'Accuracy': acc, 'F1': f1, 'Recall': recall, 'MCC': mcc}, ignore_index=True) \n",
    "results.to_csv('测试集预测_results.csv', index=False)\n",
    "\n",
    "avg_auc /= len(range(start_index, end_index)) \n",
    "avg_acc /= len(range(start_index, end_index)) \n",
    "avg_f1 /= len(range(start_index, end_index))  \n",
    "avg_recall /= len(range(start_index, end_index))\n",
    "avg_mcc /= len(range(start_index, end_index))\n",
    "\n",
    "auc_te_values = results['AUC'].values\n",
    "accuracy_te_values = results['Accuracy'].values\n",
    "f1_te_values = results['F1'].values\n",
    "recall_te_values = results['Recall'].values\n",
    "mcc_te_values = results['MCC'].values\n",
    "\n",
    "std_auc_te = np.std(auc_te_values)\n",
    "std_acc_te = np.std(accuracy_te_values)\n",
    "std_f1_te = np.std(f1_te_values)\n",
    "std_recall_te = np.std(recall_te_values)\n",
    "std_mcc_te = np.std(mcc_te_values)\n",
    "\n",
    "print(f'Average AUC: {avg_auc:.6f}±{std_auc_te:.6f}')\n",
    "print(f'Average Accuracy: {avg_acc:.6f}±{std_acc_te:.6f}')\n",
    "print(f'Average F1: {avg_f1:.6f}±{std_f1_te:.6f}')\n",
    "print(f'Average Recall: {avg_recall:.6f}±{std_recall_te:.6f}')\n",
    "print(f'Average MCC: {avg_mcc:.6f}±{std_mcc_te:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df67982a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (830061736.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_408058/830061736.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    Average AUC: 0.943448±0.020126\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "weight_decay = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "weight_decay = 0.01\n",
    "\n",
    "Average AUC: 0.943448±0.020126\n",
    "Average Accuracy: 0.796512±0.098723\n",
    "Average F1: 0.831558±0.094451\n",
    "Average Recall: 0.774691±0.173856\n",
    "Average MCC: 0.624705±0.130822\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_dim = 1\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    hidden_dim3 = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "Average AUC: 0.725601±0.064932\n",
    "Average Accuracy: 0.598296±0.056470\n",
    "Average F1: 0.666462±0.026149\n",
    "Average Recall: 0.785408±0.030126\n",
    "Average MCC: 0.353964±0.151104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c28f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aba305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f984609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.8cu111",
   "language": "python",
   "name": "pytorch1.8cu111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
