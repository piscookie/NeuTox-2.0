{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eac477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [00:05<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "file = 'NA'\n",
    "\n",
    "file_prefixes = [\"smiles\",\"DMPNN\", \"ecfp\", \"mfbert\", \"padel\"]\n",
    "file_extension = \".csv\"\n",
    "\n",
    "file_prefix = f\"{file}_Feature_fusion/data_train\"\n",
    "file_extension = \".csv\"\n",
    "\n",
    "val_file_prefix = f\"{file}_Feature_fusion/data_valid\"\n",
    "val_file_extension = \".csv\"\n",
    "\n",
    "start_index = 0\n",
    "end_index = 4\n",
    "\n",
    "auc_scores = []\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(start_index, end_index + 1), desc=\"Processing files\"):  # 使用 tqdm 添加进度条  \n",
    "    combined_data_train = None\n",
    "    combined_data_valid = None\n",
    "    combined_data_test = None\n",
    "    for file_feature in file_prefixes:\n",
    "        #从randomi/中获取train，valid，test    \n",
    "        \n",
    "        file_path_train = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"train\" + file_extension\n",
    "        data_train = pd.read_csv(file_path_train) \n",
    "                \n",
    "        file_path_valid = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"valid\" + file_extension\n",
    "        data_valid = pd.read_csv(file_path_valid) \n",
    "\n",
    "        file_path_test = file + \"/\" +'random' + str(i) + \"/\" + file_feature + \"/\" + \"test\" + file_extension\n",
    "        data_test = pd.read_csv(file_path_test)   \n",
    "        \n",
    "        data_train = pd.read_csv(file_path_train)     \n",
    "        data_valid = pd.read_csv(file_path_valid)   \n",
    "        data_test  = pd.read_csv(file_path_test) \n",
    "        \n",
    "        if combined_data_train is None:\n",
    "            combined_data_train = data_train\n",
    "        if combined_data_valid is None:\n",
    "            combined_data_valid = data_valid\n",
    "        if combined_data_test is None:   \n",
    "            combined_data_test  = data_test\n",
    "        else:\n",
    "            combined_data_train = pd.concat([combined_data_train, data_train], axis=1)     \n",
    "            combined_data_valid = pd.concat([combined_data_valid, data_valid], axis=1) \n",
    "            combined_data_test = pd.concat([combined_data_test, data_test], axis=1) \n",
    "    \n",
    "    folder_path = f'{file}_Feature_fusion'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    combined_data_train.to_csv(f'{file}_Feature_fusion/data_train{i}.csv', index=False)\n",
    "    combined_data_valid.to_csv(f'{file}_Feature_fusion/data_valid{i}.csv', index=False)\n",
    "    combined_data_test.to_csv(f'{file}_Feature_fusion/data_test{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a732548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据中没有空值\n",
      "该文件的行数为：816，列数为：2712\n",
      "数据中没有空值\n",
      "该文件的行数为：816，列数为：2712\n",
      "数据中没有空值\n",
      "该文件的行数为：816，列数为：2712\n",
      "数据中没有空值\n",
      "该文件的行数为：816，列数为：2712\n",
      "数据中没有空值\n",
      "该文件的行数为：816，列数为：2712\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "file_prefix = f\"{file}_Feature_fusion/data_train\"\n",
    "file_extension = \".csv\"\n",
    "\n",
    "val_file_prefix = f\"{file}_Feature_fusion/data_valid\"\n",
    "val_file_extension = \".csv\"\n",
    "\n",
    "for i in range(start_index, end_index+1):\n",
    "    \n",
    "    # 读取数据\n",
    "    file_path = file_prefix + str(i) + file_extension\n",
    "    data = pd.read_csv(file_path)\n",
    "    null_indices = None  # 初始化 null_indices 为 None\n",
    "    if data.isnull().values.any():  # 判断是否有空值\n",
    "        print(\"数据中存在空值，空值坐标为：\")\n",
    "        null_indices = np.where(data.isnull())  # 获取空值的坐标\n",
    "        if null_indices is not None:  # 再次判断 null_indices 是否有值\n",
    "            for row_index, col_index in zip(null_indices[0], null_indices[1]):\n",
    "                print(f\"行 {row_index}，列 {col_index}\")\n",
    "    else:\n",
    "        print(\"数据中没有空值\")\n",
    "    print(f\"该文件的行数为：{data.shape[0]}，列数为：{data.shape[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "802b00cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练第0组epochs\n",
      "loss tensor(0.2541, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2541419267654419, Val Loss: 0.23977328836917877\n",
      "model_0训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2398, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.23977328836917877, Val Loss: 0.2253342866897583\n",
      "model_0训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2253, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.2253342866897583, Val Loss: 0.2183333784341812\n",
      "model_0训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2183, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.2183333784341812, Val Loss: 0.22224198281764984\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2222, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.22224198281764984, Val Loss: 0.22438795864582062\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2244, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.22438795864582062, Val Loss: 0.22177940607070923\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2218, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.22177940607070923, Val Loss: 0.2184842824935913\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.2185, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.2184842824935913, Val Loss: 0.21729491651058197\n",
      "model_0训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.2173, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.21729491651058197, Val Loss: 0.21816369891166687\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.2182, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.21816369891166687, Val Loss: 0.21905505657196045\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.2191, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.21905505657196045, Val Loss: 0.2189672440290451\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.2190, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.2189672440290451, Val Loss: 0.21800313889980316\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.2180, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.21800313889980316, Val Loss: 0.21692050993442535\n",
      "model_0训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.2169, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.21692050993442535, Val Loss: 0.21627306938171387\n",
      "model_0训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.2163, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.21627306938171387, Val Loss: 0.21622100472450256\n",
      "model_0训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.2162, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.21622100472450256, Val Loss: 0.21643134951591492\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.2164, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.21643134951591492, Val Loss: 0.21637102961540222\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.2164, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.21637102961540222, Val Loss: 0.21593321859836578\n",
      "model_0训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.2159, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.21593321859836578, Val Loss: 0.21535207331180573\n",
      "model_0训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.2154, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.21535207331180573, Val Loss: 0.21491976082324982\n",
      "model_0训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.2149, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.21491976082324982, Val Loss: 0.21479880809783936\n",
      "model_0训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.2148, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.21479880809783936, Val Loss: 0.21474295854568481\n",
      "model_0训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.2147, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.21474295854568481, Val Loss: 0.21440087258815765\n",
      "model_0训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.2144, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.21440087258815765, Val Loss: 0.21387788653373718\n",
      "model_0训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.2139, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.21387788653373718, Val Loss: 0.21339121460914612\n",
      "model_0训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.2134, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.21339121460914612, Val Loss: 0.21307554841041565\n",
      "model_0训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.2131, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.21307554841041565, Val Loss: 0.21267832815647125\n",
      "model_0训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.2127, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.21267832815647125, Val Loss: 0.21208669245243073\n",
      "model_0训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.2121, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.21208669245243073, Val Loss: 0.21144160628318787\n",
      "model_0训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.2114, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.21144160628318787, Val Loss: 0.21088020503520966\n",
      "model_0训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.2109, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.21088020503520966, Val Loss: 0.21025975048542023\n",
      "model_0训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.2103, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.21025975048542023, Val Loss: 0.20929072797298431\n",
      "model_0训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.2093, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.20929072797298431, Val Loss: 0.20829840004444122\n",
      "model_0训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.2083, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.20829840004444122, Val Loss: 0.20727919042110443\n",
      "model_0训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.2073, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.20727919042110443, Val Loss: 0.20589491724967957\n",
      "model_0训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.2059, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.20589491724967957, Val Loss: 0.20455841720104218\n",
      "model_0训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.2046, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.20455841720104218, Val Loss: 0.2027391940355301\n",
      "model_0训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.2027, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.2027391940355301, Val Loss: 0.20091505348682404\n",
      "model_0训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.2009, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.20091505348682404, Val Loss: 0.1984565556049347\n",
      "model_0训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.1985, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.1984565556049347, Val Loss: 0.19560272991657257\n",
      "model_0训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.1956, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.19560272991657257, Val Loss: 0.19251786172389984\n",
      "model_0训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.1925, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.19251786172389984, Val Loss: 0.19132328033447266\n",
      "model_0训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.1913, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.19132328033447266, Val Loss: 0.19350269436836243\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.1935, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.19350269436836243, Val Loss: 0.18144452571868896\n",
      "model_0训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.1814, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.18144452571868896, Val Loss: 0.1915348470211029\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.1915, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.1915348470211029, Val Loss: 0.18341535329818726\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.1834, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.18341535329818726, Val Loss: 0.18093255162239075\n",
      "model_0训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.1809, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.18093255162239075, Val Loss: 0.1743365228176117\n",
      "model_0训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.1743, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.1743365228176117, Val Loss: 0.16623081266880035\n",
      "model_0训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.1662, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.16623081266880035, Val Loss: 0.1701640486717224\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.1702, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.1701640486717224, Val Loss: 0.15626336634159088\n",
      "model_0训练结果更新为第50个模型\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.1563, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.15626336634159088, Val Loss: 0.17076115310192108\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.1708, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.17076115310192108, Val Loss: 0.15530329942703247\n",
      "model_0训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.1553, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.15530329942703247, Val Loss: 0.15158064663410187\n",
      "model_0训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.1516, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.15158064663410187, Val Loss: 0.15355445444583893\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.1536, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.15355445444583893, Val Loss: 0.13324831426143646\n",
      "model_0训练结果更新为第55个模型\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.1332, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.13324831426143646, Val Loss: 0.13798727095127106\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.1380, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.13798727095127106, Val Loss: 0.12852169573307037\n",
      "model_0训练结果更新为第57个模型\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.1285, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.12852169573307037, Val Loss: 0.11775228381156921\n",
      "model_0训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.1178, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.11775228381156921, Val Loss: 0.11819013953208923\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.1182, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.11819013953208923, Val Loss: 0.11576076596975327\n",
      "model_0训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.1158, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.11576076596975327, Val Loss: 0.11159300059080124\n",
      "model_0训练结果更新为第61个模型\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.1116, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.11159300059080124, Val Loss: 0.09749735891819\n",
      "model_0训练结果更新为第62个模型\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0975, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.09749735891819, Val Loss: 0.08939655870199203\n",
      "model_0训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0894, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.08939655870199203, Val Loss: 0.08601576089859009\n",
      "model_0训练结果更新为第64个模型\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0860, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.08601576089859009, Val Loss: 0.09407203644514084\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0941, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.09407203644514084, Val Loss: 0.1472986340522766\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.1473, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.1472986340522766, Val Loss: 0.07060220837593079\n",
      "model_0训练结果更新为第67个模型\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0706, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.07060220837593079, Val Loss: 0.10709378868341446\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.1071, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.10709378868341446, Val Loss: 0.17596611380577087\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.1760, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.17596611380577087, Val Loss: 0.13217996060848236\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.1322, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.13217996060848236, Val Loss: 0.16222772002220154\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.1622, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.16222772002220154, Val Loss: 0.055827051401138306\n",
      "model_0训练结果更新为第72个模型\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0558, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.055827051401138306, Val Loss: 0.11037584394216537\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.1104, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.11037584394216537, Val Loss: 0.06193926930427551\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0619, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.06193926930427551, Val Loss: 0.11250598728656769\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.1125, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.11250598728656769, Val Loss: 0.05050978064537048\n",
      "model_0训练结果更新为第76个模型\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0505, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.05050978064537048, Val Loss: 0.08454642444849014\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0845, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.08454642444849014, Val Loss: 0.052119918167591095\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0521, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.052119918167591095, Val Loss: 0.07812367379665375\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0781, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.07812367379665375, Val Loss: 0.04369240999221802\n",
      "model_0训练结果更新为第80个模型\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0437, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.04369240999221802, Val Loss: 0.06280654668807983\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.06280654668807983, Val Loss: 0.04473590478301048\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0447, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.04473590478301048, Val Loss: 0.054759908467531204\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0548, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.054759908467531204, Val Loss: 0.03838544338941574\n",
      "model_0训练结果更新为第84个模型\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0384, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.03838544338941574, Val Loss: 0.047132257372140884\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0471, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.047132257372140884, Val Loss: 0.03652651980519295\n",
      "model_0训练结果更新为第86个模型\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0365, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.03652651980519295, Val Loss: 0.04221893474459648\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0422, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.04221893474459648, Val Loss: 0.03074461780488491\n",
      "model_0训练结果更新为第88个模型\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0307, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.03074461780488491, Val Loss: 0.03667066618800163\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0367, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.03667066618800163, Val Loss: 0.027471309527754784\n",
      "model_0训练结果更新为第90个模型\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0275, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.027471309527754784, Val Loss: 0.03341727703809738\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0334, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.03341727703809738, Val Loss: 0.02383578009903431\n",
      "model_0训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.02383578009903431, Val Loss: 0.02852676436305046\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0285, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.02852676436305046, Val Loss: 0.021269293501973152\n",
      "model_0训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0213, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.021269293501973152, Val Loss: 0.024970030412077904\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0250, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.024970030412077904, Val Loss: 0.020050562918186188\n",
      "model_0训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0201, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.020050562918186188, Val Loss: 0.020715288817882538\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0207, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.020715288817882538, Val Loss: 0.0184937696903944\n",
      "model_0训练结果更新为第98个模型\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0185, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.0184937696903944, Val Loss: 0.016933951526880264\n",
      "model_0训练结果更新为第99个模型\n",
      "         \n",
      "第 0 组模型\n",
      "AUC: 1.0\n",
      "ACC: 0.9975490196078431\n",
      "F1: 0.9961832061068702\n",
      "Recall: 0.9923954372623575\n",
      "MCC: 0.9943939045913541\n",
      "    \n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2442, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2442142516374588, Val Loss: 0.22724172472953796\n",
      "model_1训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2272, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.22724172472953796, Val Loss: 0.2173910290002823\n",
      "model_1训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2174, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.2173910290002823, Val Loss: 0.21627040207386017\n",
      "model_1训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2163, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.21627040207386017, Val Loss: 0.21216492354869843\n",
      "model_1训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2122, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.21216492354869843, Val Loss: 0.20690883696079254\n",
      "model_1训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2069, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.20690883696079254, Val Loss: 0.20114915072917938\n",
      "model_1训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2011, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.20114915072917938, Val Loss: 0.19529087841510773\n",
      "model_1训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1953, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.19529087841510773, Val Loss: 0.18772603571414948\n",
      "model_1训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1877, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.18772603571414948, Val Loss: 0.17851890623569489\n",
      "model_1训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.1785, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.17851890623569489, Val Loss: 0.16914401948451996\n",
      "model_1训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.1691, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.16914401948451996, Val Loss: 0.1586512327194214\n",
      "model_1训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.1587, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.1586512327194214, Val Loss: 0.14764143526554108\n",
      "model_1训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.1476, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.14764143526554108, Val Loss: 0.13683001697063446\n",
      "model_1训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1368, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.13683001697063446, Val Loss: 0.12716488540172577\n",
      "model_1训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1272, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.12716488540172577, Val Loss: 0.11747346818447113\n",
      "model_1训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1175, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.11747346818447113, Val Loss: 0.10779473930597305\n",
      "model_1训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1078, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.10779473930597305, Val Loss: 0.09825042635202408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0983, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.09825042635202408, Val Loss: 0.08886905014514923\n",
      "model_1训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0889, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.08886905014514923, Val Loss: 0.0788295567035675\n",
      "model_1训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0788, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.0788295567035675, Val Loss: 0.06887280195951462\n",
      "model_1训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0689, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.06887280195951462, Val Loss: 0.05889429897069931\n",
      "model_1训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0589, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.05889429897069931, Val Loss: 0.04945957288146019\n",
      "model_1训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0495, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.04945957288146019, Val Loss: 0.04151732847094536\n",
      "model_1训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0415, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.04151732847094536, Val Loss: 0.03543076291680336\n",
      "model_1训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0354, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.03543076291680336, Val Loss: 0.031150691211223602\n",
      "model_1训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0312, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.031150691211223602, Val Loss: 0.028022771701216698\n",
      "model_1训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0280, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.028022771701216698, Val Loss: 0.025631003081798553\n",
      "model_1训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0256, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.025631003081798553, Val Loss: 0.023605436086654663\n",
      "model_1训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0236, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.023605436086654663, Val Loss: 0.02171488292515278\n",
      "model_1训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0217, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.02171488292515278, Val Loss: 0.02000296302139759\n",
      "model_1训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.02000296302139759, Val Loss: 0.018565606325864792\n",
      "model_1训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0186, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.018565606325864792, Val Loss: 0.01737002469599247\n",
      "model_1训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0174, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.01737002469599247, Val Loss: 0.01624445803463459\n",
      "model_1训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.01624445803463459, Val Loss: 0.015070799738168716\n",
      "model_1训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.015070799738168716, Val Loss: 0.01399148814380169\n",
      "model_1训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.01399148814380169, Val Loss: 0.013135407119989395\n",
      "model_1训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.013135407119989395, Val Loss: 0.012338722124695778\n",
      "model_1训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0123, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.012338722124695778, Val Loss: 0.011493626981973648\n",
      "model_1训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.011493626981973648, Val Loss: 0.010736484080553055\n",
      "model_1训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0107, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.010736484080553055, Val Loss: 0.010115958750247955\n",
      "model_1训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.010115958750247955, Val Loss: 0.00950364675372839\n",
      "model_1训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.00950364675372839, Val Loss: 0.008870448917150497\n",
      "model_1训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.008870448917150497, Val Loss: 0.008316864259541035\n",
      "model_1训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.008316864259541035, Val Loss: 0.007861383259296417\n",
      "model_1训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.007861383259296417, Val Loss: 0.007476458791643381\n",
      "model_1训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.007476458791643381, Val Loss: 0.007200260180979967\n",
      "model_1训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.007200260180979967, Val Loss: 0.007152608595788479\n",
      "model_1训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.007152608595788479, Val Loss: 0.007869647815823555\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.007869647815823555, Val Loss: 0.010501059703528881\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.010501059703528881, Val Loss: 0.019196173176169395\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0192, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.019196173176169395, Val Loss: 0.019932080060243607\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.019932080060243607, Val Loss: 0.005179553758352995\n",
      "model_1训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.005179553758352995, Val Loss: 0.014887992292642593\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.014887992292642593, Val Loss: 0.004895705264061689\n",
      "model_1训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.004895705264061689, Val Loss: 0.01344096940010786\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.01344096940010786, Val Loss: 0.004746547434478998\n",
      "model_1训练结果更新为第55个模型\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.004746547434478998, Val Loss: 0.00893467664718628\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.00893467664718628, Val Loss: 0.0068156360648572445\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.0068156360648572445, Val Loss: 0.004659781698137522\n",
      "model_1训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.004659781698137522, Val Loss: 0.008293314836919308\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.008293314836919308, Val Loss: 0.004738532472401857\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.004738532472401857, Val Loss: 0.00504859397187829\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.00504859397187829, Val Loss: 0.006715230643749237\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.006715230643749237, Val Loss: 0.004343488719314337\n",
      "model_1训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.004343488719314337, Val Loss: 0.004756104201078415\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.004756104201078415, Val Loss: 0.005817749537527561\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Train Loss: 0.005817749537527561, Val Loss: 0.004005169030278921\n",
      "model_1训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.004005169030278921, Val Loss: 0.004622716922312975\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.004622716922312975, Val Loss: 0.005140708293765783\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.005140708293765783, Val Loss: 0.0038907427806407213\n",
      "model_1训练结果更新为第69个模型\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.0038907427806407213, Val Loss: 0.004348070826381445\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.004348070826381445, Val Loss: 0.004647604189813137\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.004647604189813137, Val Loss: 0.0036930993665009737\n",
      "model_1训练结果更新为第72个模型\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.0036930993665009737, Val Loss: 0.0043069738894701\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.0043069738894701, Val Loss: 0.0041826399974524975\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.0041826399974524975, Val Loss: 0.00360743491910398\n",
      "model_1训练结果更新为第75个模型\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.00360743491910398, Val Loss: 0.004235691390931606\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.004235691390931606, Val Loss: 0.0037271149922162294\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.0037271149922162294, Val Loss: 0.00372425839304924\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.00372425839304924, Val Loss: 0.00400421442463994\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.00400421442463994, Val Loss: 0.0035207169130444527\n",
      "model_1训练结果更新为第80个模型\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.0035207169130444527, Val Loss: 0.0038856524042785168\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.0038856524042785168, Val Loss: 0.003647002624347806\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.003647002624347806, Val Loss: 0.003626213874667883\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.003626213874667883, Val Loss: 0.003787388326600194\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.003787388326600194, Val Loss: 0.003499228972941637\n",
      "model_1训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.003499228972941637, Val Loss: 0.0037673884071409702\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.0037673884071409702, Val Loss: 0.0035081859678030014\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.0035081859678030014, Val Loss: 0.0036422121338546276\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.0036422121338546276, Val Loss: 0.003555798437446356\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.003555798437446356, Val Loss: 0.003520823083817959\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.003520823083817959, Val Loss: 0.0035680902656167746\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.0035680902656167746, Val Loss: 0.003447082359343767\n",
      "model_1训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.003447082359343767, Val Loss: 0.0035397398751229048\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.0035397398751229048, Val Loss: 0.003395850071683526\n",
      "model_1训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.003395850071683526, Val Loss: 0.0034910531248897314\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.0034910531248897314, Val Loss: 0.0033582206815481186\n",
      "model_1训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.0033582206815481186, Val Loss: 0.0034238200169056654\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.0034238200169056654, Val Loss: 0.003322738455608487\n",
      "model_1训练结果更新为第98个模型\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.003322738455608487, Val Loss: 0.0033575466368347406\n",
      "         \n",
      "第 1 组模型\n",
      "AUC: 0.9986454802357002\n",
      "ACC: 0.9987745098039216\n",
      "F1: 0.9980952380952381\n",
      "Recall: 0.9961977186311787\n",
      "MCC: 0.9971958322232021\n",
      "    \n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2414, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.24135984480381012, Val Loss: 0.22848394513130188\n",
      "model_2训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2285, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.22848394513130188, Val Loss: 0.21562540531158447\n",
      "model_2训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2156, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.21562540531158447, Val Loss: 0.21288514137268066\n",
      "model_2训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2129, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.21288514137268066, Val Loss: 0.20987123250961304\n",
      "model_2训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2099, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.20987123250961304, Val Loss: 0.20012201368808746\n",
      "model_2训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2001, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.20012201368808746, Val Loss: 0.19297505915164948\n",
      "model_2训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1930, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.19297505915164948, Val Loss: 0.18695133924484253\n",
      "model_2训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1870, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.18695133924484253, Val Loss: 0.17368918657302856\n",
      "model_2训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1737, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.17368918657302856, Val Loss: 0.16234134137630463\n",
      "model_2训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.1623, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.16234134137630463, Val Loss: 0.15063580870628357\n",
      "model_2训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.1506, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.15063580870628357, Val Loss: 0.13573728501796722\n",
      "model_2训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.1357, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.13573728501796722, Val Loss: 0.12374154478311539\n",
      "model_2训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.1237, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.12374154478311539, Val Loss: 0.10999880731105804\n",
      "model_2训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1100, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.10999880731105804, Val Loss: 0.09849760681390762\n",
      "model_2训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.0985, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.09849760681390762, Val Loss: 0.08886774629354477\n",
      "model_2训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.0889, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.08886774629354477, Val Loss: 0.07850581407546997\n",
      "model_2训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.0785, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.07850581407546997, Val Loss: 0.07054869085550308\n",
      "model_2训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0705, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.07054869085550308, Val Loss: 0.06166509911417961\n",
      "model_2训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0617, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.06166509911417961, Val Loss: 0.054947782307863235\n",
      "model_2训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0549, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.054947782307863235, Val Loss: 0.0481695793569088\n",
      "model_2训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0482, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.0481695793569088, Val Loss: 0.04334103316068649\n",
      "model_2训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0433, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.04334103316068649, Val Loss: 0.03865659609436989\n",
      "model_2训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0387, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.03865659609436989, Val Loss: 0.03543345257639885\n",
      "model_2训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0354, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.03543345257639885, Val Loss: 0.03232024982571602\n",
      "model_2训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0323, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.03232024982571602, Val Loss: 0.030227309092879295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0302, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.030227309092879295, Val Loss: 0.02802090346813202\n",
      "model_2训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0280, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.02802090346813202, Val Loss: 0.026532689109444618\n",
      "model_2训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0265, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.026532689109444618, Val Loss: 0.024904638528823853\n",
      "model_2训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0249, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.024904638528823853, Val Loss: 0.0238017775118351\n",
      "model_2训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.0238017775118351, Val Loss: 0.02240240015089512\n",
      "model_2训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0224, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.02240240015089512, Val Loss: 0.021391797810792923\n",
      "model_2训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0214, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.021391797810792923, Val Loss: 0.0200277641415596\n",
      "model_2训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.0200277641415596, Val Loss: 0.01898803561925888\n",
      "model_2训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0190, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.01898803561925888, Val Loss: 0.01766689121723175\n",
      "model_2训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0177, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.01766689121723175, Val Loss: 0.016621733084321022\n",
      "model_2训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0166, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.016621733084321022, Val Loss: 0.015495630912482738\n",
      "model_2训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.015495630912482738, Val Loss: 0.01446782611310482\n",
      "model_2训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.01446782611310482, Val Loss: 0.013653979636728764\n",
      "model_2训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.013653979636728764, Val Loss: 0.012754268944263458\n",
      "model_2训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0128, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.012754268944263458, Val Loss: 0.012102066539227962\n",
      "model_2训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.012102066539227962, Val Loss: 0.01149851456284523\n",
      "model_2训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.01149851456284523, Val Loss: 0.010865298099815845\n",
      "model_2训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.010865298099815845, Val Loss: 0.010414774529635906\n",
      "model_2训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.010414774529635906, Val Loss: 0.009994781576097012\n",
      "model_2训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.009994781576097012, Val Loss: 0.009524784982204437\n",
      "model_2训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.009524784982204437, Val Loss: 0.009160133078694344\n",
      "model_2训练结果更新为第45个模型\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.009160133078694344, Val Loss: 0.008906456641852856\n",
      "model_2训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.008906456641852856, Val Loss: 0.008672111667692661\n",
      "model_2训练结果更新为第47个模型\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.008672111667692661, Val Loss: 0.008406716398894787\n",
      "model_2训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.008406716398894787, Val Loss: 0.008138510398566723\n",
      "model_2训练结果更新为第49个模型\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.008138510398566723, Val Loss: 0.007867058739066124\n",
      "model_2训练结果更新为第50个模型\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.007867058739066124, Val Loss: 0.007600527256727219\n",
      "model_2训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.007600527256727219, Val Loss: 0.007356051355600357\n",
      "model_2训练结果更新为第52个模型\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.007356051355600357, Val Loss: 0.00719355558976531\n",
      "model_2训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.00719355558976531, Val Loss: 0.007446596398949623\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.007446596398949623, Val Loss: 0.01034853607416153\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.01034853607416153, Val Loss: 0.023630280047655106\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0236, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.023630280047655106, Val Loss: 0.03022594563663006\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0302, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.03022594563663006, Val Loss: 0.007330934051424265\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.007330934051424265, Val Loss: 0.027714921161532402\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0277, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.027714921161532402, Val Loss: 0.006471376400440931\n",
      "model_2训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.006471376400440931, Val Loss: 0.024592822417616844\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0246, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.024592822417616844, Val Loss: 0.007433714345097542\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.007433714345097542, Val Loss: 0.011555570177733898\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.011555570177733898, Val Loss: 0.016029810532927513\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0160, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.016029810532927513, Val Loss: 0.0069358376786112785\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.0069358376786112785, Val Loss: 0.009966237470507622\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.009966237470507622, Val Loss: 0.013309085741639137\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.013309085741639137, Val Loss: 0.007099818903952837\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.007099818903952837, Val Loss: 0.007400226779282093\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.007400226779282093, Val Loss: 0.011001012288033962\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0110, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.011001012288033962, Val Loss: 0.0073911407962441444\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.0073911407962441444, Val Loss: 0.005928972736001015\n",
      "model_2训练结果更新为第72个模型\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.005928972736001015, Val Loss: 0.008322001434862614\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.008322001434862614, Val Loss: 0.007967188023030758\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.007967188023030758, Val Loss: 0.00546383997425437\n",
      "model_2训练结果更新为第75个模型\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.00546383997425437, Val Loss: 0.005943908356130123\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.005943908356130123, Val Loss: 0.0072875237092375755\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.0072875237092375755, Val Loss: 0.005525572225451469\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.005525572225451469, Val Loss: 0.004829851910471916\n",
      "model_2训练结果更新为第79个模型\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.004829851910471916, Val Loss: 0.00610851775854826\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.00610851775854826, Val Loss: 0.005583697464317083\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.005583697464317083, Val Loss: 0.00435620779171586\n",
      "model_2训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.00435620779171586, Val Loss: 0.005141513422131538\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.005141513422131538, Val Loss: 0.00526330154389143\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.00526330154389143, Val Loss: 0.0041776797734200954\n",
      "model_2训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.0041776797734200954, Val Loss: 0.004674092400819063\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.004674092400819063, Val Loss: 0.004908788483589888\n",
      "         \n",
      "开始训练第88组epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.004908788483589888, Val Loss: 0.004082086496055126\n",
      "model_2训练结果更新为第88个模型\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.004082086496055126, Val Loss: 0.004442066885530949\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.004442066885530949, Val Loss: 0.004588370677083731\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.004588370677083731, Val Loss: 0.003997118677943945\n",
      "model_2训练结果更新为第91个模型\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.003997118677943945, Val Loss: 0.0043671829625964165\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.0043671829625964165, Val Loss: 0.004343749489635229\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.004343749489635229, Val Loss: 0.00397432129830122\n",
      "model_2训练结果更新为第94个模型\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.00397432129830122, Val Loss: 0.0043394858948886395\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.0043394858948886395, Val Loss: 0.0041503990069031715\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.0041503990069031715, Val Loss: 0.004020406398922205\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.004020406398922205, Val Loss: 0.004287998657673597\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.004287998657673597, Val Loss: 0.00399251701310277\n",
      "         \n",
      "第 2 组模型\n",
      "AUC: 0.9982739857066607\n",
      "ACC: 0.9963235294117647\n",
      "F1: 0.9942196531791908\n",
      "Recall: 0.9961389961389961\n",
      "MCC: 0.9915282072778575\n",
      "    \n",
      "开始训练第0组epochs\n",
      "loss tensor(0.2622, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.2622416317462921, Val Loss: 0.23851807415485382\n",
      "model_3训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2385, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.23851807415485382, Val Loss: 0.219850093126297\n",
      "model_3训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2199, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.219850093126297, Val Loss: 0.21614161133766174\n",
      "model_3训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2161, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.21614161133766174, Val Loss: 0.21786288917064667\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2179, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.21786288917064667, Val Loss: 0.21258851885795593\n",
      "model_3训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2126, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.21258851885795593, Val Loss: 0.2049882560968399\n",
      "model_3训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.2050, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.2049882560968399, Val Loss: 0.2006145715713501\n",
      "model_3训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.2006, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.2006145715713501, Val Loss: 0.1975303292274475\n",
      "model_3训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1975, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.1975303292274475, Val Loss: 0.19084715843200684\n",
      "model_3训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.1908, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.19084715843200684, Val Loss: 0.18284043669700623\n",
      "model_3训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.1828, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.18284043669700623, Val Loss: 0.17464058101177216\n",
      "model_3训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.1746, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.17464058101177216, Val Loss: 0.16596609354019165\n",
      "model_3训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.1660, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.16596609354019165, Val Loss: 0.15609610080718994\n",
      "model_3训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1561, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.15609610080718994, Val Loss: 0.1464492827653885\n",
      "model_3训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1464, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.1464492827653885, Val Loss: 0.13611528277397156\n",
      "model_3训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1361, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.13611528277397156, Val Loss: 0.126418799161911\n",
      "model_3训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1264, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.126418799161911, Val Loss: 0.11785891652107239\n",
      "model_3训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.1179, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.11785891652107239, Val Loss: 0.1095658391714096\n",
      "model_3训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.1096, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.1095658391714096, Val Loss: 0.10157327353954315\n",
      "model_3训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.1016, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.10157327353954315, Val Loss: 0.09354522079229355\n",
      "model_3训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0935, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.09354522079229355, Val Loss: 0.0855550691485405\n",
      "model_3训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0856, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.0855550691485405, Val Loss: 0.07773961871862411\n",
      "model_3训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0777, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.07773961871862411, Val Loss: 0.07007038593292236\n",
      "model_3训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0701, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.07007038593292236, Val Loss: 0.06359091401100159\n",
      "model_3训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0636, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.06359091401100159, Val Loss: 0.05799935758113861\n",
      "model_3训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0580, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.05799935758113861, Val Loss: 0.05303586274385452\n",
      "model_3训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0530, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.05303586274385452, Val Loss: 0.04870700463652611\n",
      "model_3训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0487, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.04870700463652611, Val Loss: 0.045089613646268845\n",
      "model_3训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0451, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.045089613646268845, Val Loss: 0.04201571270823479\n",
      "model_3训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0420, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.04201571270823479, Val Loss: 0.03934797644615173\n",
      "model_3训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0393, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.03934797644615173, Val Loss: 0.03741556778550148\n",
      "model_3训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0374, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.03741556778550148, Val Loss: 0.03711993619799614\n",
      "model_3训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0371, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.03711993619799614, Val Loss: 0.03726010397076607\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0373, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.03726010397076607, Val Loss: 0.031080881133675575\n",
      "model_3训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0311, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.031080881133675575, Val Loss: 0.028866613283753395\n",
      "model_3训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0289, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.028866613283753395, Val Loss: 0.02963470295071602\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0296, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.02963470295071602, Val Loss: 0.024436868727207184\n",
      "model_3训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0244, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.024436868727207184, Val Loss: 0.024473706260323524\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0245, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.024473706260323524, Val Loss: 0.023072199895977974\n",
      "model_3训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.023072199895977974, Val Loss: 0.019799185916781425\n",
      "model_3训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0198, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.019799185916781425, Val Loss: 0.020709093660116196\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0207, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.020709093660116196, Val Loss: 0.01775727979838848\n",
      "model_3训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0178, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.01775727979838848, Val Loss: 0.01750565879046917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_3训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0175, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.01750565879046917, Val Loss: 0.01674840785562992\n",
      "model_3训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0167, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.01674840785562992, Val Loss: 0.01493618544191122\n",
      "model_3训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.01493618544191122, Val Loss: 0.015402335673570633\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.015402335673570633, Val Loss: 0.013603326864540577\n",
      "model_3训练结果更新为第46个模型\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.013603326864540577, Val Loss: 0.013816272839903831\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.013816272839903831, Val Loss: 0.013071145862340927\n",
      "model_3训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.013071145862340927, Val Loss: 0.012244555167853832\n",
      "model_3训练结果更新为第49个模型\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0122, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.012244555167853832, Val Loss: 0.012522687204182148\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.012522687204182148, Val Loss: 0.01131998561322689\n",
      "model_3训练结果更新为第51个模型\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0113, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.01131998561322689, Val Loss: 0.011426975019276142\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0114, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.011426975019276142, Val Loss: 0.011002173647284508\n",
      "model_3训练结果更新为第53个模型\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0110, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.011002173647284508, Val Loss: 0.010371936485171318\n",
      "model_3训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.010371936485171318, Val Loss: 0.01054051611572504\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "Epoch 57/100, Train Loss: 0.01054051611572504, Val Loss: 0.00989084504544735\n",
      "model_3训练结果更新为第56个模型\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.00989084504544735, Val Loss: 0.009755623526871204\n",
      "model_3训练结果更新为第57个模型\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.009755623526871204, Val Loss: 0.009729436598718166\n",
      "model_3训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.009729436598718166, Val Loss: 0.009130490012466908\n",
      "model_3训练结果更新为第59个模型\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.009130490012466908, Val Loss: 0.009084070101380348\n",
      "model_3训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.009084070101380348, Val Loss: 0.009074246510863304\n",
      "model_3训练结果更新为第61个模型\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.009074246510863304, Val Loss: 0.008536490611732006\n",
      "model_3训练结果更新为第62个模型\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.008536490611732006, Val Loss: 0.008227609097957611\n",
      "model_3训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.008227609097957611, Val Loss: 0.00824129581451416\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.00824129581451416, Val Loss: 0.008056658320128918\n",
      "model_3训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.008056658320128918, Val Loss: 0.007645503152161837\n",
      "model_3训练结果更新为第66个模型\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.007645503152161837, Val Loss: 0.007178846746683121\n",
      "model_3训练结果更新为第67个模型\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.007178846746683121, Val Loss: 0.006857461296021938\n",
      "model_3训练结果更新为第68个模型\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.006857461296021938, Val Loss: 0.006686741951853037\n",
      "model_3训练结果更新为第69个模型\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.006686741951853037, Val Loss: 0.006662976462393999\n",
      "model_3训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.006662976462393999, Val Loss: 0.00708930566906929\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.00708930566906929, Val Loss: 0.00888896081596613\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.00888896081596613, Val Loss: 0.02092244289815426\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0209, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.02092244289815426, Val Loss: 0.08094941824674606\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0809, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.08094941824674606, Val Loss: 0.07716885954141617\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0772, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.07716885954141617, Val Loss: 0.03271558880805969\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0327, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.03271558880805969, Val Loss: 0.024591851979494095\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0246, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.024591851979494095, Val Loss: 0.05234377831220627\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0523, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.05234377831220627, Val Loss: 0.007738292682915926\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.007738292682915926, Val Loss: 0.06441821902990341\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0644, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.06441821902990341, Val Loss: 0.018121931701898575\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0181, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.018121931701898575, Val Loss: 0.00847497209906578\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.00847497209906578, Val Loss: 0.04514208063483238\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0451, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.04514208063483238, Val Loss: 0.012483254075050354\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.012483254075050354, Val Loss: 0.008158496581017971\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.008158496581017971, Val Loss: 0.02377828024327755\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.02377828024327755, Val Loss: 0.021490884944796562\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0215, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.021490884944796562, Val Loss: 0.008166602812707424\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.008166602812707424, Val Loss: 0.0076073757372796535\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.0076073757372796535, Val Loss: 0.01725754141807556\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0173, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.01725754141807556, Val Loss: 0.0135503513738513\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.0135503513738513, Val Loss: 0.0061169820837676525\n",
      "model_3训练结果更新为第92个模型\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.0061169820837676525, Val Loss: 0.006998920347541571\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.006998920347541571, Val Loss: 0.011003406718373299\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0110, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.011003406718373299, Val Loss: 0.011503458954393864\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.011503458954393864, Val Loss: 0.0076001789420843124\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.0076001789420843124, Val Loss: 0.004926038905978203\n",
      "model_3训练结果更新为第97个模型\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.004926038905978203, Val Loss: 0.00574470404535532\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.00574470404535532, Val Loss: 0.00846850872039795\n",
      "         \n",
      "第 3 组模型\n",
      "AUC: 0.9992011891861855\n",
      "ACC: 0.9963235294117647\n",
      "F1: 0.9941972920696325\n",
      "Recall: 0.9961240310077519\n",
      "MCC: 0.9915106973714605\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练第0组epochs\n",
      "loss tensor(0.2434, grad_fn=<MseLossBackward>)\n",
      "Epoch 1/100, Train Loss: 0.24342194199562073, Val Loss: 0.228919118642807\n",
      "model_4训练结果更新为第0个模型\n",
      "         \n",
      "开始训练第1组epochs\n",
      "loss tensor(0.2289, grad_fn=<MseLossBackward>)\n",
      "Epoch 2/100, Train Loss: 0.228919118642807, Val Loss: 0.21233181655406952\n",
      "model_4训练结果更新为第1个模型\n",
      "         \n",
      "开始训练第2组epochs\n",
      "loss tensor(0.2123, grad_fn=<MseLossBackward>)\n",
      "Epoch 3/100, Train Loss: 0.21233181655406952, Val Loss: 0.20612682402133942\n",
      "model_4训练结果更新为第2个模型\n",
      "         \n",
      "开始训练第3组epochs\n",
      "loss tensor(0.2061, grad_fn=<MseLossBackward>)\n",
      "Epoch 4/100, Train Loss: 0.20612682402133942, Val Loss: 0.20600906014442444\n",
      "model_4训练结果更新为第3个模型\n",
      "         \n",
      "开始训练第4组epochs\n",
      "loss tensor(0.2060, grad_fn=<MseLossBackward>)\n",
      "Epoch 5/100, Train Loss: 0.20600906014442444, Val Loss: 0.2022566944360733\n",
      "model_4训练结果更新为第4个模型\n",
      "         \n",
      "开始训练第5组epochs\n",
      "loss tensor(0.2023, grad_fn=<MseLossBackward>)\n",
      "Epoch 6/100, Train Loss: 0.2022566944360733, Val Loss: 0.19615380465984344\n",
      "model_4训练结果更新为第5个模型\n",
      "         \n",
      "开始训练第6组epochs\n",
      "loss tensor(0.1962, grad_fn=<MseLossBackward>)\n",
      "Epoch 7/100, Train Loss: 0.19615380465984344, Val Loss: 0.190043106675148\n",
      "model_4训练结果更新为第6个模型\n",
      "         \n",
      "开始训练第7组epochs\n",
      "loss tensor(0.1900, grad_fn=<MseLossBackward>)\n",
      "Epoch 8/100, Train Loss: 0.190043106675148, Val Loss: 0.18466949462890625\n",
      "model_4训练结果更新为第7个模型\n",
      "         \n",
      "开始训练第8组epochs\n",
      "loss tensor(0.1847, grad_fn=<MseLossBackward>)\n",
      "Epoch 9/100, Train Loss: 0.18466949462890625, Val Loss: 0.17746461927890778\n",
      "model_4训练结果更新为第8个模型\n",
      "         \n",
      "开始训练第9组epochs\n",
      "loss tensor(0.1775, grad_fn=<MseLossBackward>)\n",
      "Epoch 10/100, Train Loss: 0.17746461927890778, Val Loss: 0.16873563826084137\n",
      "model_4训练结果更新为第9个模型\n",
      "         \n",
      "开始训练第10组epochs\n",
      "loss tensor(0.1687, grad_fn=<MseLossBackward>)\n",
      "Epoch 11/100, Train Loss: 0.16873563826084137, Val Loss: 0.1596875786781311\n",
      "model_4训练结果更新为第10个模型\n",
      "         \n",
      "开始训练第11组epochs\n",
      "loss tensor(0.1597, grad_fn=<MseLossBackward>)\n",
      "Epoch 12/100, Train Loss: 0.1596875786781311, Val Loss: 0.14856182038784027\n",
      "model_4训练结果更新为第11个模型\n",
      "         \n",
      "开始训练第12组epochs\n",
      "loss tensor(0.1486, grad_fn=<MseLossBackward>)\n",
      "Epoch 13/100, Train Loss: 0.14856182038784027, Val Loss: 0.13696110248565674\n",
      "model_4训练结果更新为第12个模型\n",
      "         \n",
      "开始训练第13组epochs\n",
      "loss tensor(0.1370, grad_fn=<MseLossBackward>)\n",
      "Epoch 14/100, Train Loss: 0.13696110248565674, Val Loss: 0.12488866597414017\n",
      "model_4训练结果更新为第13个模型\n",
      "         \n",
      "开始训练第14组epochs\n",
      "loss tensor(0.1249, grad_fn=<MseLossBackward>)\n",
      "Epoch 15/100, Train Loss: 0.12488866597414017, Val Loss: 0.11256410926580429\n",
      "model_4训练结果更新为第14个模型\n",
      "         \n",
      "开始训练第15组epochs\n",
      "loss tensor(0.1126, grad_fn=<MseLossBackward>)\n",
      "Epoch 16/100, Train Loss: 0.11256410926580429, Val Loss: 0.10155003517866135\n",
      "model_4训练结果更新为第15个模型\n",
      "         \n",
      "开始训练第16组epochs\n",
      "loss tensor(0.1016, grad_fn=<MseLossBackward>)\n",
      "Epoch 17/100, Train Loss: 0.10155003517866135, Val Loss: 0.09037715196609497\n",
      "model_4训练结果更新为第16个模型\n",
      "         \n",
      "开始训练第17组epochs\n",
      "loss tensor(0.0904, grad_fn=<MseLossBackward>)\n",
      "Epoch 18/100, Train Loss: 0.09037715196609497, Val Loss: 0.0805387794971466\n",
      "model_4训练结果更新为第17个模型\n",
      "         \n",
      "开始训练第18组epochs\n",
      "loss tensor(0.0805, grad_fn=<MseLossBackward>)\n",
      "Epoch 19/100, Train Loss: 0.0805387794971466, Val Loss: 0.07084009796380997\n",
      "model_4训练结果更新为第18个模型\n",
      "         \n",
      "开始训练第19组epochs\n",
      "loss tensor(0.0708, grad_fn=<MseLossBackward>)\n",
      "Epoch 20/100, Train Loss: 0.07084009796380997, Val Loss: 0.06275447458028793\n",
      "model_4训练结果更新为第19个模型\n",
      "         \n",
      "开始训练第20组epochs\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward>)\n",
      "Epoch 21/100, Train Loss: 0.06275447458028793, Val Loss: 0.05536780133843422\n",
      "model_4训练结果更新为第20个模型\n",
      "         \n",
      "开始训练第21组epochs\n",
      "loss tensor(0.0554, grad_fn=<MseLossBackward>)\n",
      "Epoch 22/100, Train Loss: 0.05536780133843422, Val Loss: 0.04949501156806946\n",
      "model_4训练结果更新为第21个模型\n",
      "         \n",
      "开始训练第22组epochs\n",
      "loss tensor(0.0495, grad_fn=<MseLossBackward>)\n",
      "Epoch 23/100, Train Loss: 0.04949501156806946, Val Loss: 0.044595230370759964\n",
      "model_4训练结果更新为第22个模型\n",
      "         \n",
      "开始训练第23组epochs\n",
      "loss tensor(0.0446, grad_fn=<MseLossBackward>)\n",
      "Epoch 24/100, Train Loss: 0.044595230370759964, Val Loss: 0.04075532779097557\n",
      "model_4训练结果更新为第23个模型\n",
      "         \n",
      "开始训练第24组epochs\n",
      "loss tensor(0.0408, grad_fn=<MseLossBackward>)\n",
      "Epoch 25/100, Train Loss: 0.04075532779097557, Val Loss: 0.037781309336423874\n",
      "model_4训练结果更新为第24个模型\n",
      "         \n",
      "开始训练第25组epochs\n",
      "loss tensor(0.0378, grad_fn=<MseLossBackward>)\n",
      "Epoch 26/100, Train Loss: 0.037781309336423874, Val Loss: 0.03503702953457832\n",
      "model_4训练结果更新为第25个模型\n",
      "         \n",
      "开始训练第26组epochs\n",
      "loss tensor(0.0350, grad_fn=<MseLossBackward>)\n",
      "Epoch 27/100, Train Loss: 0.03503702953457832, Val Loss: 0.0329376719892025\n",
      "model_4训练结果更新为第26个模型\n",
      "         \n",
      "开始训练第27组epochs\n",
      "loss tensor(0.0329, grad_fn=<MseLossBackward>)\n",
      "Epoch 28/100, Train Loss: 0.0329376719892025, Val Loss: 0.03092644177377224\n",
      "model_4训练结果更新为第27个模型\n",
      "         \n",
      "开始训练第28组epochs\n",
      "loss tensor(0.0309, grad_fn=<MseLossBackward>)\n",
      "Epoch 29/100, Train Loss: 0.03092644177377224, Val Loss: 0.029147343710064888\n",
      "model_4训练结果更新为第28个模型\n",
      "         \n",
      "开始训练第29组epochs\n",
      "loss tensor(0.0291, grad_fn=<MseLossBackward>)\n",
      "Epoch 30/100, Train Loss: 0.029147343710064888, Val Loss: 0.02766142599284649\n",
      "model_4训练结果更新为第29个模型\n",
      "         \n",
      "开始训练第30组epochs\n",
      "loss tensor(0.0277, grad_fn=<MseLossBackward>)\n",
      "Epoch 31/100, Train Loss: 0.02766142599284649, Val Loss: 0.026134932413697243\n",
      "model_4训练结果更新为第30个模型\n",
      "         \n",
      "开始训练第31组epochs\n",
      "loss tensor(0.0261, grad_fn=<MseLossBackward>)\n",
      "Epoch 32/100, Train Loss: 0.026134932413697243, Val Loss: 0.024839838966727257\n",
      "model_4训练结果更新为第31个模型\n",
      "         \n",
      "开始训练第32组epochs\n",
      "loss tensor(0.0248, grad_fn=<MseLossBackward>)\n",
      "Epoch 33/100, Train Loss: 0.024839838966727257, Val Loss: 0.02359895408153534\n",
      "model_4训练结果更新为第32个模型\n",
      "         \n",
      "开始训练第33组epochs\n",
      "loss tensor(0.0236, grad_fn=<MseLossBackward>)\n",
      "Epoch 34/100, Train Loss: 0.02359895408153534, Val Loss: 0.02233785018324852\n",
      "model_4训练结果更新为第33个模型\n",
      "         \n",
      "开始训练第34组epochs\n",
      "loss tensor(0.0223, grad_fn=<MseLossBackward>)\n",
      "Epoch 35/100, Train Loss: 0.02233785018324852, Val Loss: 0.021204011514782906\n",
      "model_4训练结果更新为第34个模型\n",
      "         \n",
      "开始训练第35组epochs\n",
      "loss tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "Epoch 36/100, Train Loss: 0.021204011514782906, Val Loss: 0.020017048344016075\n",
      "model_4训练结果更新为第35个模型\n",
      "         \n",
      "开始训练第36组epochs\n",
      "loss tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "Epoch 37/100, Train Loss: 0.020017048344016075, Val Loss: 0.01886194571852684\n",
      "model_4训练结果更新为第36个模型\n",
      "         \n",
      "开始训练第37组epochs\n",
      "loss tensor(0.0189, grad_fn=<MseLossBackward>)\n",
      "Epoch 38/100, Train Loss: 0.01886194571852684, Val Loss: 0.01777944155037403\n",
      "model_4训练结果更新为第37个模型\n",
      "         \n",
      "开始训练第38组epochs\n",
      "loss tensor(0.0178, grad_fn=<MseLossBackward>)\n",
      "Epoch 39/100, Train Loss: 0.01777944155037403, Val Loss: 0.016667451709508896\n",
      "model_4训练结果更新为第38个模型\n",
      "         \n",
      "开始训练第39组epochs\n",
      "loss tensor(0.0167, grad_fn=<MseLossBackward>)\n",
      "Epoch 40/100, Train Loss: 0.016667451709508896, Val Loss: 0.015596257522702217\n",
      "model_4训练结果更新为第39个模型\n",
      "         \n",
      "开始训练第40组epochs\n",
      "loss tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "Epoch 41/100, Train Loss: 0.015596257522702217, Val Loss: 0.014606554992496967\n",
      "model_4训练结果更新为第40个模型\n",
      "         \n",
      "开始训练第41组epochs\n",
      "loss tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "Epoch 42/100, Train Loss: 0.014606554992496967, Val Loss: 0.013656921684741974\n",
      "model_4训练结果更新为第41个模型\n",
      "         \n",
      "开始训练第42组epochs\n",
      "loss tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "Epoch 43/100, Train Loss: 0.013656921684741974, Val Loss: 0.01274876855313778\n",
      "model_4训练结果更新为第42个模型\n",
      "         \n",
      "开始训练第43组epochs\n",
      "loss tensor(0.0127, grad_fn=<MseLossBackward>)\n",
      "Epoch 44/100, Train Loss: 0.01274876855313778, Val Loss: 0.012031247839331627\n",
      "model_4训练结果更新为第43个模型\n",
      "         \n",
      "开始训练第44组epochs\n",
      "loss tensor(0.0120, grad_fn=<MseLossBackward>)\n",
      "Epoch 45/100, Train Loss: 0.012031247839331627, Val Loss: 0.011907408013939857\n",
      "model_4训练结果更新为第44个模型\n",
      "         \n",
      "开始训练第45组epochs\n",
      "loss tensor(0.0119, grad_fn=<MseLossBackward>)\n",
      "Epoch 46/100, Train Loss: 0.011907408013939857, Val Loss: 0.013841149397194386\n",
      "         \n",
      "开始训练第46组epochs\n",
      "loss tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "Epoch 47/100, Train Loss: 0.013841149397194386, Val Loss: 0.015635354444384575\n",
      "         \n",
      "开始训练第47组epochs\n",
      "loss tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "Epoch 48/100, Train Loss: 0.015635354444384575, Val Loss: 0.01308470405638218\n",
      "         \n",
      "开始训练第48组epochs\n",
      "loss tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "Epoch 49/100, Train Loss: 0.01308470405638218, Val Loss: 0.009238389320671558\n",
      "model_4训练结果更新为第48个模型\n",
      "         \n",
      "开始训练第49组epochs\n",
      "loss tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "Epoch 50/100, Train Loss: 0.009238389320671558, Val Loss: 0.012568981386721134\n",
      "         \n",
      "开始训练第50组epochs\n",
      "loss tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "Epoch 51/100, Train Loss: 0.012568981386721134, Val Loss: 0.008615072816610336\n",
      "model_4训练结果更新为第50个模型\n",
      "         \n",
      "开始训练第51组epochs\n",
      "loss tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch 52/100, Train Loss: 0.008615072816610336, Val Loss: 0.010876472108066082\n",
      "         \n",
      "开始训练第52组epochs\n",
      "loss tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "Epoch 53/100, Train Loss: 0.010876472108066082, Val Loss: 0.00863829255104065\n",
      "         \n",
      "开始训练第53组epochs\n",
      "loss tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch 54/100, Train Loss: 0.00863829255104065, Val Loss: 0.009661884047091007\n",
      "         \n",
      "开始训练第54组epochs\n",
      "loss tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch 55/100, Train Loss: 0.009661884047091007, Val Loss: 0.008594926446676254\n",
      "model_4训练结果更新为第54个模型\n",
      "         \n",
      "开始训练第55组epochs\n",
      "loss tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch 56/100, Train Loss: 0.008594926446676254, Val Loss: 0.008869988843798637\n",
      "         \n",
      "开始训练第56组epochs\n",
      "loss tensor(0.0089, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Train Loss: 0.008869988843798637, Val Loss: 0.007739965803921223\n",
      "model_4训练结果更新为第56个模型\n",
      "         \n",
      "开始训练第57组epochs\n",
      "loss tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "Epoch 58/100, Train Loss: 0.007739965803921223, Val Loss: 0.008114921860396862\n",
      "         \n",
      "开始训练第58组epochs\n",
      "loss tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "Epoch 59/100, Train Loss: 0.008114921860396862, Val Loss: 0.0073682889342308044\n",
      "model_4训练结果更新为第58个模型\n",
      "         \n",
      "开始训练第59组epochs\n",
      "loss tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "Epoch 60/100, Train Loss: 0.0073682889342308044, Val Loss: 0.007581790909171104\n",
      "         \n",
      "开始训练第60组epochs\n",
      "loss tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "Epoch 61/100, Train Loss: 0.007581790909171104, Val Loss: 0.006432114169001579\n",
      "model_4训练结果更新为第60个模型\n",
      "         \n",
      "开始训练第61组epochs\n",
      "loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch 62/100, Train Loss: 0.006432114169001579, Val Loss: 0.006715346593409777\n",
      "         \n",
      "开始训练第62组epochs\n",
      "loss tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "Epoch 63/100, Train Loss: 0.006715346593409777, Val Loss: 0.006384186912328005\n",
      "model_4训练结果更新为第62个模型\n",
      "         \n",
      "开始训练第63组epochs\n",
      "loss tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch 64/100, Train Loss: 0.006384186912328005, Val Loss: 0.005895270500332117\n",
      "model_4训练结果更新为第63个模型\n",
      "         \n",
      "开始训练第64组epochs\n",
      "loss tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "Epoch 65/100, Train Loss: 0.005895270500332117, Val Loss: 0.006198203656822443\n",
      "         \n",
      "开始训练第65组epochs\n",
      "loss tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch 66/100, Train Loss: 0.006198203656822443, Val Loss: 0.004754317924380302\n",
      "model_4训练结果更新为第65个模型\n",
      "         \n",
      "开始训练第66组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 67/100, Train Loss: 0.004754317924380302, Val Loss: 0.00577288493514061\n",
      "         \n",
      "开始训练第67组epochs\n",
      "loss tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch 68/100, Train Loss: 0.00577288493514061, Val Loss: 0.004561934620141983\n",
      "model_4训练结果更新为第67个模型\n",
      "         \n",
      "开始训练第68组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 69/100, Train Loss: 0.004561934620141983, Val Loss: 0.005234622396528721\n",
      "         \n",
      "开始训练第69组epochs\n",
      "loss tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "Epoch 70/100, Train Loss: 0.005234622396528721, Val Loss: 0.004845050163567066\n",
      "         \n",
      "开始训练第70组epochs\n",
      "loss tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "Epoch 71/100, Train Loss: 0.004845050163567066, Val Loss: 0.0044319601729512215\n",
      "model_4训练结果更新为第70个模型\n",
      "         \n",
      "开始训练第71组epochs\n",
      "loss tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "Epoch 72/100, Train Loss: 0.0044319601729512215, Val Loss: 0.004879671148955822\n",
      "         \n",
      "开始训练第72组epochs\n",
      "loss tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "Epoch 73/100, Train Loss: 0.004879671148955822, Val Loss: 0.004039272200316191\n",
      "model_4训练结果更新为第72个模型\n",
      "         \n",
      "开始训练第73组epochs\n",
      "loss tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "Epoch 74/100, Train Loss: 0.004039272200316191, Val Loss: 0.004555932246148586\n",
      "         \n",
      "开始训练第74组epochs\n",
      "loss tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "Epoch 75/100, Train Loss: 0.004555932246148586, Val Loss: 0.0041066911071538925\n",
      "         \n",
      "开始训练第75组epochs\n",
      "loss tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "Epoch 76/100, Train Loss: 0.0041066911071538925, Val Loss: 0.004178617149591446\n",
      "         \n",
      "开始训练第76组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 77/100, Train Loss: 0.004178617149591446, Val Loss: 0.004285994917154312\n",
      "         \n",
      "开始训练第77组epochs\n",
      "loss tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "Epoch 78/100, Train Loss: 0.004285994917154312, Val Loss: 0.0038937400095164776\n",
      "model_4训练结果更新为第77个模型\n",
      "         \n",
      "开始训练第78组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 79/100, Train Loss: 0.0038937400095164776, Val Loss: 0.004245656542479992\n",
      "         \n",
      "开始训练第79组epochs\n",
      "loss tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "Epoch 80/100, Train Loss: 0.004245656542479992, Val Loss: 0.003848911728709936\n",
      "model_4训练结果更新为第79个模型\n",
      "         \n",
      "开始训练第80组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 81/100, Train Loss: 0.003848911728709936, Val Loss: 0.0038981828838586807\n",
      "         \n",
      "开始训练第81组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 82/100, Train Loss: 0.0038981828838586807, Val Loss: 0.003919237293303013\n",
      "         \n",
      "开始训练第82组epochs\n",
      "loss tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "Epoch 83/100, Train Loss: 0.003919237293303013, Val Loss: 0.0036154973786324263\n",
      "model_4训练结果更新为第82个模型\n",
      "         \n",
      "开始训练第83组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 84/100, Train Loss: 0.0036154973786324263, Val Loss: 0.003812034148722887\n",
      "         \n",
      "开始训练第84组epochs\n",
      "loss tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "Epoch 85/100, Train Loss: 0.003812034148722887, Val Loss: 0.0036289035342633724\n",
      "         \n",
      "开始训练第85组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 86/100, Train Loss: 0.0036289035342633724, Val Loss: 0.0035609258338809013\n",
      "model_4训练结果更新为第85个模型\n",
      "         \n",
      "开始训练第86组epochs\n",
      "loss tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "Epoch 87/100, Train Loss: 0.0035609258338809013, Val Loss: 0.0036668754182755947\n",
      "         \n",
      "开始训练第87组epochs\n",
      "loss tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "Epoch 88/100, Train Loss: 0.0036668754182755947, Val Loss: 0.0034512868151068687\n",
      "model_4训练结果更新为第87个模型\n",
      "         \n",
      "开始训练第88组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 89/100, Train Loss: 0.0034512868151068687, Val Loss: 0.003483991138637066\n",
      "         \n",
      "开始训练第89组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 90/100, Train Loss: 0.003483991138637066, Val Loss: 0.003488450078293681\n",
      "         \n",
      "开始训练第90组epochs\n",
      "loss tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "Epoch 91/100, Train Loss: 0.003488450078293681, Val Loss: 0.0033199614845216274\n",
      "model_4训练结果更新为第90个模型\n",
      "         \n",
      "开始训练第91组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 92/100, Train Loss: 0.0033199614845216274, Val Loss: 0.0033828339073807\n",
      "         \n",
      "开始训练第92组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 93/100, Train Loss: 0.0033828339073807, Val Loss: 0.003350242041051388\n",
      "         \n",
      "开始训练第93组epochs\n",
      "loss tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "Epoch 94/100, Train Loss: 0.003350242041051388, Val Loss: 0.0032257770653814077\n",
      "model_4训练结果更新为第93个模型\n",
      "         \n",
      "开始训练第94组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 95/100, Train Loss: 0.0032257770653814077, Val Loss: 0.003276915056630969\n",
      "         \n",
      "开始训练第95组epochs\n",
      "loss tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "Epoch 96/100, Train Loss: 0.003276915056630969, Val Loss: 0.003232842544093728\n",
      "         \n",
      "开始训练第96组epochs\n",
      "loss tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "Epoch 97/100, Train Loss: 0.003232842544093728, Val Loss: 0.003121145535260439\n",
      "model_4训练结果更新为第96个模型\n",
      "         \n",
      "开始训练第97组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 98/100, Train Loss: 0.003121145535260439, Val Loss: 0.0031460237223654985\n",
      "         \n",
      "开始训练第98组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 99/100, Train Loss: 0.0031460237223654985, Val Loss: 0.0031145382672548294\n",
      "model_4训练结果更新为第98个模型\n",
      "         \n",
      "开始训练第99组epochs\n",
      "loss tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "Epoch 100/100, Train Loss: 0.0031145382672548294, Val Loss: 0.003014447633177042\n",
      "model_4训练结果更新为第99个模型\n",
      "         \n",
      "第 4 组模型\n",
      "AUC: 0.9986609842485674\n",
      "ACC: 0.9987745098039216\n",
      "F1: 0.9979296066252588\n",
      "Recall: 0.9958677685950413\n",
      "MCC: 0.9970636011055957\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "for i in range(start_index, end_index+1):\n",
    "    \n",
    "    # 读取数据\n",
    "    file_path = file_prefix + str(i) + file_extension\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 提取特征和标签\n",
    "    x_input = data.drop(['SMILES'], axis=1).values\n",
    "    y_output = data['activity'].values\n",
    "    \n",
    "        # 遍历数据，将 inf 赋值为 0\n",
    "    x_input = np.nan_to_num(x_input, posinf=0, neginf=0)\n",
    "    \n",
    "#        # 检查并输出非 float64 数据的坐标\n",
    "#    for row_index, row in enumerate(x_input):\n",
    "#        for col_index, value in enumerate(row):\n",
    "#            if x_input.dtype!= np.float64:\n",
    "#                print(f\"非 float64 数据位于 ({row_index}, {col_index})，值为: {value}\")\n",
    "#    \n",
    "    #print('x_input',x_input)\n",
    "    #print('y_output',y_output)\n",
    "    \n",
    "    \n",
    "    val_x_input = data.drop(['SMILES'], axis=1).values\n",
    "    val_y_output = data['activity'].values\n",
    "    \n",
    "    val_x_input = np.nan_to_num(x_input, posinf=0, neginf=0)\n",
    "    \n",
    "#    # 查找 x_input 中的无穷值位置\n",
    "#    infinity_positions_x = np.argwhere(np.isinf(x_input))\n",
    "#    if len(infinity_positions_x) > 0:\n",
    "#        column_names = data.drop(['SMILES'], axis=1).columns\n",
    "#        infinity_column_names_x = [column_names[pos[1]] for pos in infinity_positions_x]\n",
    "#        print(\"在 x_input 中的无穷值所在列名：\", infinity_column_names_x, '         位置：',infinity_positions_x)\n",
    "#    else:\n",
    "#        print(\"在 x_input 中没有无穷值\")\n",
    "#\n",
    "#    # 查找 val_x_input 中的无穷值位置\n",
    "#    infinity_positions_val_x = np.argwhere(np.isinf(val_x_input))\n",
    "#    if len(infinity_positions_val_x) > 0:\n",
    "#        column_names = data.drop(['SMILES'], axis=1).columns\n",
    "#        infinity_column_names_val_x = [column_names[pos[1]] for pos in infinity_positions_val_x]\n",
    "#        print(\"在 val_x_input 中的无穷值所在列名：\", infinity_column_names_val_x, '        位置：',infinity_positions_x)\n",
    "#    else:\n",
    "#        print(\"在 val_x_input 中没有无穷值\")\n",
    "    \n",
    "        # 标准化特征（假设特征是数值型的）\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    x_input = scaler.fit_transform(x_input)\n",
    "    val_x_input = scaler.fit_transform(val_x_input)\n",
    "\n",
    "    # 数据放缩处理（例如，将特征值缩放到 0 到 1 之间）\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_input = min_max_scaler.fit_transform(x_input)\n",
    "    val_x_input = min_max_scaler.fit_transform(val_x_input)\n",
    "    \n",
    "        # 转换为Tensor\n",
    "    x = torch.Tensor(x_input)\n",
    "    y_true = torch.Tensor(y_output).view(-1, 1)    \n",
    "    \n",
    "    val_x = torch.Tensor(val_x_input)\n",
    "    val_y_true = torch.Tensor(val_y_output).view(-1, 1)\n",
    "    \n",
    "    \n",
    "    # 数据维度\n",
    "    input_dim = x.shape[1]\n",
    "    output_dim = 1\n",
    "    hidden_dim1 = 256\n",
    "    hidden_dim2 = 128\n",
    "    hidden_dim3 = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    weight_decay = 0.001\n",
    "    # MLP模型定义\n",
    "    mlp = nn.Sequential(  \n",
    "        nn.Linear(input_dim, hidden_dim1),  # 第一层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim1, hidden_dim2),  # 第二层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim2, hidden_dim3),  # 第三层隐藏层  \n",
    "        nn.ReLU(),  \n",
    "        nn.Linear(hidden_dim3, output_dim),  # 输出层  \n",
    "        nn.Sigmoid()  \n",
    "    )  \n",
    "        \n",
    "    # 优化器和损失函数\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "    #loss_func =  nn.BCEWithLogitsLoss()\n",
    "    loss_func =  nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    num_iterations_without_improvement = 0  # Initialize counter for iterations without improvement. \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(f\"开始训练第{epoch}组epochs\")\n",
    "        prediction = mlp(x)\n",
    "        #print('prediction',prediction)\n",
    "        \n",
    "        #prediction_np = prediction.detach().numpy()\n",
    "        #df = pd.DataFrame(prediction_np)\n",
    "        #df.to_csv('prediction.csv', index=False)\n",
    "        \n",
    "        loss = loss_func(prediction, y_true)\n",
    "        print('loss',loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(optimizer)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_prediction=mlp(val_x)\n",
    "            val_loss = loss_func(val_prediction, val_y_true)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "        \n",
    "        #print(\"检查数据和模型输出:\")\n",
    "        #print(\"是否存在 NaN 或无穷大值在训练预测结果中:\", torch.isnan(prediction).any() or torch.isinf(prediction).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在验证预测结果中:\", torch.isnan(val_prediction).any() or torch.isinf(val_prediction).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在训练真实值中:\", torch.isnan(y_true).any() or torch.isinf(y_true).any())\n",
    "        #print(\"是否存在 NaN 或无穷大值在验证真实值中:\", torch.isnan(val_y_true).any() or torch.isinf(val_y_true).any())\n",
    "        #\n",
    "          \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  \n",
    "            num_iterations_without_improvement = 0\n",
    "            torch.save(mlp.state_dict(), f\"models_{file}/model_{i+1}.pt\")\n",
    "            #torch.save(mlp, \"model_saved/mlp1.pt\")\n",
    "            print(f'model_{i}训练结果更新为第{epoch}个模型')\n",
    "            best_epoch = epoch\n",
    "        else :\n",
    "            num_iterations_without_improvement += 1  \n",
    "            if num_iterations_without_improvement == 300:\n",
    "                print('300次迭代没有更新，结束迭代')\n",
    "                print(f'最棒的模型是第{best_epoch}个epoch')\n",
    "                break\n",
    "        print('         ')\n",
    "        \n",
    "\n",
    "    prediction_np = prediction.data.numpy()\n",
    "        # 将 prediction 添加到 DataFrame\n",
    "    #data['prediction'] = prediction_np\n",
    "        # 将 DataFrame 保存到 CSV 文件\n",
    "    #data.to_csv('output/output_train.csv', index=True)\n",
    "    \n",
    "    # 预测并计算AUC\n",
    "    prediction_np = prediction.detach().numpy().flatten()\n",
    "    auc = roc_auc_score(y_true, prediction_np)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    binary_prediction = np.where(prediction_np > threshold, 1, 0)\n",
    "    \n",
    "    acc = accuracy_score(y_true, binary_prediction)\n",
    "    f1 = f1_score(y_true, binary_prediction)\n",
    "    recall = recall_score(y_true, binary_prediction)\n",
    "    mcc = matthews_corrcoef(y_true, binary_prediction)\n",
    "    output_file = \"train_scores.csv\"\n",
    "    \n",
    "    \n",
    "    auc_scores.append(auc)\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    recall_scores.append(recall)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    print('第',i,'组模型')\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"ACC:\", acc)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print('    ')\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "972b13e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_1: 0.8813291139240507\n",
      "Accuracy_1: 0.8932038834951457\n",
      "f1_1: 0.744186046511628\n",
      "recall_1: 0.6666666666666666\n",
      "mcc_1: 0.6852367480867336\n",
      "             \n",
      "             \n",
      "AUC_2: 0.8583333333333334\n",
      "Accuracy_2: 0.7647058823529411\n",
      "f1_2: 0.6666666666666666\n",
      "recall_2: 0.8\n",
      "mcc_2: 0.5092010548749033\n",
      "             \n",
      "             \n",
      "AUC_3: 0.8571428571428571\n",
      "Accuracy_3: 0.7843137254901961\n",
      "f1_3: 0.6666666666666667\n",
      "recall_3: 0.6875\n",
      "mcc_3: 0.5079721589671172\n",
      "             \n",
      "             \n",
      "AUC_4: 0.8129401408450704\n",
      "Accuracy_4: 0.7475728155339806\n",
      "f1_4: 0.6486486486486486\n",
      "recall_4: 0.75\n",
      "mcc_4: 0.46753634073087225\n",
      "             \n",
      "             \n",
      "Average AUC: 0.852436±0.024757\n",
      "Average Accuracy: 0.797449±0.056792\n",
      "Average F1: 0.681542±0.036908\n",
      "Average Recall: 0.726042±0.052571\n",
      "Average MCC: 0.542487±0.084105\n"
     ]
    }
   ],
   "source": [
    "te_file_prefix = f\"{file}_Feature_fusion/data_test\"\n",
    "te_file_extension = \".csv\"\n",
    "\n",
    "import os \n",
    "\n",
    "avg_auc = 0.0  \n",
    "avg_acc = 0.0  \n",
    "avg_f1 = 0.0  \n",
    "avg_recall = 0.0  \n",
    "avg_mcc = 0.0  \n",
    "\n",
    "\n",
    "#te_file_paths = [te_file_prefix + str(i) + te_file_extension for i in range(start_index, end_index+1)]  \n",
    "results = pd.DataFrame(columns=['AUC', 'Accuracy', 'F1', 'Recall', 'MCC'])   \n",
    "    \n",
    "#for i,te_file_path in enumerate(te_file_paths):  \n",
    "for i in range(start_index, end_index): \n",
    "    te_file_path = te_file_prefix + str(i) + te_file_extension\n",
    "    data_test = pd.read_csv(te_file_path)  \n",
    "    if file == 'BBB':\n",
    "        data_test = data_test.head(129)  \n",
    "\n",
    "    #print(data_test)\n",
    "      \n",
    "    te_x_input = data_test.drop(['SMILES'], axis=1).values  \n",
    "    #print('1te_x_input.shape',te_x_input.shape)\n",
    "    \n",
    "    te_y_true = data_test['activity'].values  \n",
    "    te_x_input = te_x_input.astype(np.float64)\n",
    "    #print('2te_x_input.shape',te_x_input.shape)\n",
    "    \n",
    "    te_x_input = np.nan_to_num(te_x_input, posinf=0, neginf=0)\n",
    "    #print('3te_x_input',te_x_input.shape)    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    te_x_input = scaler.fit_transform(te_x_input)\n",
    "\n",
    "    # 数据放缩处理（例如，将特征值缩放到 0 到 1 之间）\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    te_x_input = min_max_scaler.fit_transform(te_x_input)\n",
    "    te_x_input = torch.Tensor(te_x_input)\n",
    "    #print('4te_x_input.shape',te_x_input.shape)\n",
    "    mlp.load_state_dict(torch.load(f\"models_{file}/model_{i+1}.pt\")) \n",
    "    \n",
    "    #print('39 te_x_input te_x_input ',te_x_input)\n",
    "    \n",
    "    #print(\"Length of te_prediction:\", len(te_prediction))\n",
    "    #print(\"Length of data_test:\", len(data_test))\n",
    "    #print(\"Length of data index:\", len(data.index))\n",
    "    #\n",
    "    with torch.no_grad():    \n",
    "        \n",
    "       # print(\"47  te_x_input  te_x_input\",te_x_input)\n",
    "        \n",
    "        te_prediction = mlp(te_x_input)  \n",
    "        \n",
    "        ############################################################\n",
    "        te_prediction = te_prediction.numpy()\n",
    "        data = pd.DataFrame(te_prediction)\n",
    "        data.to_csv(f'output1/test_prediction{i+1}.csv', mode='a', index=False)\n",
    "        ############################################################\n",
    "        \n",
    "        #print('te_y_true',len(te_y_true))\n",
    "        #print('te_prediction',len(te_prediction))\n",
    "        #print('te_y_true',te_y_true)\n",
    "        #print('te_prediction',te_prediction)\n",
    "        #print('检查 te_y_true 是否包含 NaN',np.isnan(te_y_true).any())  # 检查 te_y_true 是否包含 NaN\n",
    "        #print('检查 te_y_true 是否包含无穷值',np.isinf(te_y_true).any())  # 检查 te_y_true 是否包含无穷值\n",
    "        #print('检查 te_prediction 是否包含 NaN',np.isnan(te_prediction).any())  # 检查 te_prediction 是否包含 NaN\n",
    "        #print('检查 te_prediction 是否包含无穷值',np.isinf(te_prediction).any())  # 检查 te_prediction 是否包含无穷值\n",
    "        #\n",
    "        auc = roc_auc_score(te_y_true, te_prediction)\n",
    "        #print(auc)\n",
    "        \n",
    "        #print('54  te_predictionte_prediction',te_prediction)    \n",
    "        #te_prediction = te_prediction.numpy()      \n",
    "        data_test[f'te_prediction_{i+1}'] = te_prediction \n",
    "        \n",
    "        auc = roc_auc_score(te_y_true, te_prediction)\n",
    "        #print(auc)\n",
    "        \n",
    "        #data_test.to_csv(f'output1/te_prediction{i+1}.csv', mode='a', index=True)\n",
    "        #print('te_prediction  ',te_prediction)\n",
    "        \n",
    "        auc = roc_auc_score(te_y_true, te_prediction)  \n",
    "        print(f'AUC_{i+1}: {auc}')\n",
    "        \n",
    "        te_prediction_binary = np.where(te_prediction > 0.5, 1, 0)  \n",
    "        acc = accuracy_score(te_y_true, te_prediction_binary)  \n",
    "        print(f'Accuracy_{i+1}: {acc}')\n",
    "        f1 = f1_score(te_y_true, te_prediction_binary)\n",
    "        recall = recall_score(te_y_true, te_prediction_binary)\n",
    "        mcc = matthews_corrcoef(te_y_true, te_prediction_binary)\n",
    "        print(f'f1_{i+1}: {f1}')\n",
    "        print(f'recall_{i+1}: {recall}')\n",
    "        print(f'mcc_{i+1}: {mcc}')\n",
    "        print('             ')\n",
    "        print('             ')\n",
    "        \n",
    "        avg_auc += auc  \n",
    "        avg_acc += acc  \n",
    "        avg_f1 += f1  \n",
    "        avg_recall += recall  \n",
    "        avg_mcc += mcc\n",
    "        \n",
    "        results = results.append({'AUC': auc, 'Accuracy': acc, 'F1': f1, 'Recall': recall, 'MCC': mcc}, ignore_index=True) \n",
    "results.to_csv('测试集预测_results.csv', index=False)\n",
    "\n",
    "avg_auc /= len(range(start_index, end_index)) \n",
    "avg_acc /= len(range(start_index, end_index)) \n",
    "avg_f1 /= len(range(start_index, end_index))  \n",
    "avg_recall /= len(range(start_index, end_index))\n",
    "avg_mcc /= len(range(start_index, end_index))\n",
    "\n",
    "auc_te_values = results['AUC'].values\n",
    "accuracy_te_values = results['Accuracy'].values\n",
    "f1_te_values = results['F1'].values\n",
    "recall_te_values = results['Recall'].values\n",
    "mcc_te_values = results['MCC'].values\n",
    "\n",
    "std_auc_te = np.std(auc_te_values)\n",
    "std_acc_te = np.std(accuracy_te_values)\n",
    "std_f1_te = np.std(f1_te_values)\n",
    "std_recall_te = np.std(recall_te_values)\n",
    "std_mcc_te = np.std(mcc_te_values)\n",
    "\n",
    "print(f'Average AUC: {avg_auc:.6f}±{std_auc_te:.6f}')\n",
    "print(f'Average Accuracy: {avg_acc:.6f}±{std_acc_te:.6f}')\n",
    "print(f'Average F1: {avg_f1:.6f}±{std_f1_te:.6f}')\n",
    "print(f'Average Recall: {avg_recall:.6f}±{std_recall_te:.6f}')\n",
    "print(f'Average MCC: {avg_mcc:.6f}±{std_mcc_te:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df67982a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (830061736.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_359856/830061736.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    Average AUC: 0.943448±0.020126\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "weight_decay = 0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "weight_decay = 0.01\n",
    "\n",
    "Average AUC: 0.943448±0.020126\n",
    "Average Accuracy: 0.796512±0.098723\n",
    "Average F1: 0.831558±0.094451\n",
    "Average Recall: 0.774691±0.173856\n",
    "Average MCC: 0.624705±0.130822\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    output_dim = 1\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    hidden_dim3 = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "Average AUC: 0.725601±0.064932\n",
    "Average Accuracy: 0.598296±0.056470\n",
    "Average F1: 0.666462±0.026149\n",
    "Average Recall: 0.785408±0.030126\n",
    "Average MCC: 0.353964±0.151104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c28f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aba305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f984609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.8cu111",
   "language": "python",
   "name": "pytorch1.8cu111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
